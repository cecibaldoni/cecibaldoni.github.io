{
  "hash": "27f2e59b0902b641b12883d5b1506fe1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Is Note-Taking as powerful as I think? (2/2)\"\ndescription: \"Part 2 of 2 - This post shows how I dived into my digital journal to extract information about my writing style, my weekly mood, and yes, make more plots.\"\ndate: 2025-08-19\nimage: img/grampasimpson.gif\ncategories: [PKM, note-taking]\n---\n\n# Is Note-Taking as Powerful as I Think? (A text mining journey)\n\nIn the first part of this series, I made a plot.\n\nWell ok. It was a bit more than that. It was a nice exploration of my work tasks based on self-recorded notes, and in the end… yes, I got a nice, clean plot that put my skills and enjoyment into neat little boxes.\n\nI could have stopped there, satisfied with my color-coded chart. But here's the thing about note-taking: it's addictive. And I realized the real story wasn't in the tidy, structured scores from [Part 1](https://cecibaldoni.github.io/blog/notetaking-part1.html), but was hidden in the unstructured chaos of my daily journal… the digital equivalent of a junk drawer full of brilliant ideas, and existential questions about lunch.\n\nCould I find meaning in that mess? Let's find out.\n\n![The gif i was looking for in the previous part, with Grampa Simpson thinking back and finding only static noise](img/grampasimpsonwide.gif)\n\n*Small notes before we begin:*\n\n*1. This post is structured in* chapters*, and it's a deliberate choice. Since this whole adventure is about analyzing long-form writing, it felt right to structure the post itself like a book. We're on a literary adventure here, people!*\n\n*2. Given the sensitive information in my private journal, I decided to paste the actual R code I used, but show you the output as a static image (.png). This was a conscious choice. I tried to create some \"fake entries\" but I felt they did not really tell the story I wanted to share.*\n\n## Chapter 1: Defining a battle strategy\n\nI have to admit, I knew nothing about how to analyse long text entries. I had no idea what the process was called, or if it was possible to do it in R. I just knew I wanted to learn more about my writing: know which words I use more often, if there were interesting trends over time, or if I was getting happier (or sadder). Since I didn’t know how to put all of this into actual words, I did what I usually do in these situations: I described my goal to a Large Language Model.\n\nI should be happy they are not sentient yet, because it might have thought I was being obtuse and ridiculous.\n\nAfter writing down my question, it just replied: \"Did you mean `text mining`?\"\n\nEhm, I guess. Thank you.\n\nThe next obvious step was to find a guide. As a sucker for a good coding book, I was thrilled to find [\"Text Mining with R\"](https://www.tidytextmining.com/) online, which became my manual for this adventure.\n\n### What Logseq Gives You\n\n> *IMPORTANT COMMUNICATION*\n>\n> *Here, I need to take a pause and acknowledge something important. Despite my love for [Logseq](https://logseq.com/), not everybody uses it (say whaaaat!). I debated with myself whether to add this section at all, and jump into the text mining part with a nice fake dataset created in R for the occasion. But reality is never that easy and tidy. So… If you are following along, and you too take your long notes in Logseq (or [Obsidian](https://obsidian.md/), for that matter), you might get to similar data wrangling impasses. For that reason, I decided to keep this part. If you have never used Logseq and you are just in for the ride, this might be less interesting for you, and you might want to skip to the part where the data is tidy and ready for analysis.*\n\n\nBefore any analysis, I had to get my data out of Logseq and into R. This involves looping through all your journal files and pulling out any lines, and blocks, that contain your target tags (mine were `#journaling`, and `#moodlog`). This is very similar to the loop I wrote for the Part 1 of this series, I just swapped the tags:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nlogseq_path <- \"~/Logseq/journals\"\nmd_files <- list.files(logseq_path, pattern = \"\\\\.md$\", full.names = TRUE)\n\nextract_entries <- function(file_path) {\n  lines <- readLines(file_path, warn = FALSE)\n  file_date <- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %>% ymd()\n  \n  result <- list()\n  i <- 1\n  \n  while (i <= length(lines)) {\n    line <- lines[i]\n    if (str_detect(line, \"moodlog|journaling\")) {\n      \n      matched_tag <- str_extract(line, \"moodlog|journaling\")\n      entry_lines <- c(line)\n      i <- i + 1\n      \n      while (\n        i <= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines <- c(entry_lines, lines[i])\n        i <- i + 1\n      }\n      \n      result <- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i <- i + 1\n    }\n  }\n  if (length(result) > 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\n\nentries <- map_df(md_files, extract_entries)\n```\n:::\n\n\nAfter running this, you're left with a data frame. But it's not pretty: it's a raw data dump full of markdown syntax with weird tabs (`\\t`), newlines (`\\n`), and automated metadata.\n\nHere is an example of how it might look like, built with some fake entries:\n\n![](img/note-taking2_1.png)\n\nThere are several problems when you import this from Logseq into R. If you like Logseq for its block structure, know that that becomes your nightmare. If you add new lines, or tags, or any other plugin feature, they are all gonna be *written there*.\n\nHere is what I did to clean up this mess.\n\n**Weapon 1: `fixed()` for `str_remove_all()`**\\\nOf course, the process wasn't smooth. I hit a syntax error trying to clean a `{{renderer :wordcount_}}` tag from Logseq (side note: that’s the function I use to count the words in my long writing within Logseq. It’s pretty neat, unless you want to load the data into R). The problem is that the `str_remove_all()` function thinks I'm being difficult by giving it a complex search pattern. Apparently, the curly braces are confusing the function.\n\nThe fix? `fixed()` (*drum roll sound*), which tells R to stop trying to be clever and just find the literal text. I don't know about you, but adding \"fixed\" into a line of code makes it all better. It should be the default.\n\n**Weapon 2: Surgical strikes on metadata**\\\nThen there was the problem about the metadata. First, there was `collapsed::true`, which is just my way of telling Logseq to hide children blocks. It means if I step away from my laptop to get a coffee, my wandering thoughts don't become public reading material for anyone walking by. The other one was `:LOGBOOK: ... :END:`, metadata automatically generated by Logseq's time-tracking feature. It's meant to record how long it passed from \"assigning a task\" to completing it. They are structured metadata, so they will definitely contaminate the text analysis.\n\nWhat to do with those? **Remove the entire lines.** This is better than excluding the whole entry because it preserves any useful text that might be on the same line, while getting rid of all the noisy metadata.\n\n**Weapon 3: `str_squish()`**\\\nHave you heard about this function? It removes all the weird extra spaces from your text. It's an absolute lifesaver.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nentries <- entries %>%\n  mutate(\n    entry_text = entry_text %>%\n      str_remove(\"journaling|moodlog\") %>%\n      str_remove_all(\"\\\\n|\\\\t\") %>%\n      str_remove_all(\"-|#|\\\\|\") %>%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %>%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %>%\n      str_remove_all(\"collapsed:: true\") %>%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %>%\n      \n      str_squish()\n  )\n```\n:::\n\n\n## Chapter 2: What am I actually writing about?\n\nI had all the entries in R, I had a date column, and a text column. What now?\n\nWell, let's follow the book.\n\nThe tidy text book uses as an example Jane Austen books, from the package `janeaustenr`. Of course, using great literature would be way cooler (more words, for one, and probably better words too), but we will battle imposter syndrome and convince ourselves that our entries have nothing to compete with.\n\nWell, no offense to Jane Austen, but my own messy journal entries are the most interesting part of this project for me.\n\nThe first important step is to remove `stop_words`, meaning all the articles, prepositions and “fillers” words. You can use a specific dataset embedded in `tidytext`. Without the filler words, what remains it’s only important stuff (well, let’s see!).\n\nWhich is the most used word in my journal entries?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\n\nwords_counted <- entries %>%\n  unnest_tokens(word, entry_text) %>%\n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)\n\nwords_counted  %>% \n  filter(n > 6) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n```\n:::\n\n\n![](img/most-used-words1.svg)\n\nFun fact! From my own entries the most used word was \"Time\". Which I found very intriguing.\n\nA funnier fact? That's the second most used word in the Jane Austen books! Take that Jane Austen!\n\nActually, it's also the first word for Brontë and Horwell, isn't that interesting?\n\nWant to do this yourself? The core of it is using `unnest_tokens()` to create one word per row and `count()` to tally them up. \n\n## Chapter 3: What is the `sentiment` of my writing?\n\nKnowing *what* I write about is interesting, but the next logical step is to figure out *how I felt* when I was writing it. This is where sentiment analysis comes in. There are several different methods, or \"lexicons,\" available in R that assign positive or negative scores to words. I tried many using my dataset, but for simplicity, here I will use the one called \"AFINN\".\n\nThe AFINN lexicon is essentially a word list where each word is rated on a scale from -5 (very negative) to +5 (very positive). The process is quite simple: the code goes through each words in my journal entries, finds all the words that exist in the AFINN list, and then adds up their scores. An entry with words like \"amazing\" and \"happy\" would get a high positive score, while an entry with \"terrible\" and \"stress\" would get a negative one.\n\nThe `get_sentiment()` function from the `syuzhet` package does all this work for you, creating a single sentiment score for each day's entry.\n\nThe first thing I did was plot the sentiment score of my entries over time. This is where the story took another sharp turn.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(syuzhet)\n\nsentiment <- entries %>%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Plot sentiment over time\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n```\n:::\n\n\n![](img/sentiment-score1.svg)\n\nIs it my impression or is there a bit of a gap? How come?\n\nThen I looked at the dates and realised: that’s when I was writing my dissertation! I remember now, the tag I was using during that time was `#Dissertation Journal`, to distinguish it from other entries. I should add those entries too. Well, easy enough, I could re-run my loop, adding an additional tag!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_entries <- function(file_path) {\n  lines <- readLines(file_path, warn = FALSE)\n  file_date <- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %>% ymd()\n  \n  result <- list()\n  i <- 1\n  \n  while (i <= length(lines)) {\n    line <- lines[i]\n    if (str_detect(line, \"moodlog|journaling|Dissertation Journal\")) { #Add the additional tag!\n      \n      matched_tag <- str_extract(line, \"moodlog|journaling|Dissertation Journal\") #Here too!\n      entry_lines <- c(line)\n      i <- i + 1\n      \n      while (\n        i <= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines <- c(entry_lines, lines[i])\n        i <- i + 1\n      }\n      \n      # Save entry\n      result <- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i <- i + 1\n    }\n  }\n  if (length(result) > 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\nentries <- map_df(md_files, extract_entries)\n\nentries <- entries %>%\n  mutate(\n    entry_text = entry_text %>%\n      str_remove(\"journaling|moodlog|Dissertation Journal\") %>%\n      str_remove_all(\"\\\\n|\\\\t\") %>%\n      str_remove_all(\"-|#|\\\\|\") %>%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %>%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %>%\n      str_remove_all(\"logseq.orderlisttype::\") %>% \n      \n      str_remove_all(\"collapsed:: true\") %>%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %>% \n      \n      str_squish()\n  )\n\nsentiment <- entries %>%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Re-plot\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n```\n:::\n\n\n![](img/sentiment-score2.svg)\n\nThis seems about right! I am happy I left the tags in the dataset now, I could see the evolution of my journaling, from `moodlog` to `journaling`, with a big sidetrack project in the middle!\n\nThe tag column could also have a new meaning. I could now *FOR EXAMPLE* visualise if I was more prone to use negative words while writing my dissertation (no, we are not going there. It was just a thought).\n\nBut with these new entries, what about my most used words now?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Re-run word frequency with the new tag\n\nwords_counted <- entries %>%\n  unnest_tokens(word, entry_text) %>%\n  anti_join(stop_words) %>% \n  count(word, sort = TRUE)\n\nwords_counted  %>% \n  filter(n > 20) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n```\n:::\n\n\n![](img/most-used-words2.svg)\n\nAfter including my dissertation entries, the word frequency analysis had completely changed. My most used word was not `time` anymore, but `paper`.\n\nLOL. Yes, I literally laughed out loud on this one.\n\n### A word cloud solved everything\n\nTo understand what was driving these scores, I wanted to see the most common positive and negative words. The `tidytext` book has a fantastic visualization for this: a comparison word cloud.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npositive_negative_words <- entries %>%\n  # i need to tokenize into single words\n  unnest_tokens(word, entry_text) %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE)\n\n# We need to pivot the data into a matrix format:\n# one row per word, one column for \"positive\", one for \"negative\".\n\npositive_negative_words %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  comparison.cloud(\n    colors = c(\"firebrick\", \"steelblue\"),\n    max.words = 100\n  )\n\n#LOL shrew is a NEGATIVE word XD \n```\n:::\n\n\n![](img/word-cloud.png)\n\nThis is where things get both interesting and messy.\n\nApparently \"`shrew`\" is a negative word! I might need to thank Shakespeare for that. But when the shrew is your actual study species, the word doesn't quite have the same meaning as in [*The Taming of the Shrew*](https://en.wikipedia.org/wiki/The_Taming_of_the_Shrew), right?\n\nBut I also see other problems. For example, \"`excel`\" could mean the verb, or it could mean I was frustrated with Microsoft Excel. \"`Chronic`\" and \"`stress`\" likely refer to a study I was working on about *chronic stress*, not *my* chronic stress.\n\nAnd what about “`regression`”? I am sure I never meant it negatively, I was probably talking about statistics (wait, that does not exclude the negative connotation tho!)\n\nIt's fascinating. Have you ever wondered how many scientific terms have a negative meaning? \"Issue\", \"regression\", \"plot\"... it's a long list!\n\nI'm also amused by how often the word \"`like`\" appears on the positive side. Am I using it too much in my writing?\n\n## Chapter 4: The emotional rhythm of my week\n\nAfter wrestling with the beautiful mess of the word cloud, I could finally stop looking at individual words and start searching for broader patterns in my own emotional week.\n\nWas there a rhythm to my moods? A secret logic to my good and bad days?\n\nThis is where Logseq's structure really shines. Since every journal entry is automatically tied to a daily note, getting the day of the week is incredibly simple with the `wday()` function from the `lubridate` package. I decided to look at the average sentiment for each day of the week, and that's when I found it. A clear, undeniable pattern.\n\nApparently, my week has a villain, and its name is `Friday`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate) # For wday()\n\n# It should have columns for 'date' and 'sentiment_score'\n\nworkweek_sentiment <- sentiment %>%\n  mutate(day_of_week = wday(date, label = TRUE, week_start = 1)) %>%\n  group_by(day_of_week) %>%\n  filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %>% #let's remove saturday and sunday!\n  summarise(\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(workweek_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n\nweek_sentiment <- sentiment %>%\n  mutate(\n    # Get the day of the week from the date\n    day_of_week = wday(date, label = TRUE, week_start = 1)\n  ) %>%\n  group_by(day_of_week) %>%\n  #filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %>% #let's remove saturday and sunday!\n  summarise(\n    # Calculate the average sentiment for each day\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(week_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  #geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n```\n:::\n\n\n![](img/weekdays.svg)\n\n![](img/week.svg)\n\n*Side note: If you are wondering about the weeknames, just know they are in German!*\n\nMy mood consistently takes a nosedive at the end of the work week, hitting a low on Friday. But the story gets more interesting when you look at the weekend. The sentiment shoots up dramatically on Sundays. The contrast was so stark that I ended up making two plots: one showing just the weekdays, and another including the weekend for comparison.\n\nIt was one of those moments where data confirms a vague feeling you've had for years.\n\nWho knew? Well, a part of me did, I guess. But seeing it as a data point makes it real. It's no longer just \"a bad day\"; it's a predictable rhythm. And once you see the pattern, you can start to do something about it. This is a discovery I can now be mindful of: I could, for example, schedule lighter tasks for Fridays or just be a little kinder to myself when the end-of-week slump hits.\n\n### A Curious Detour - The story in my \"filler\" words\n\nAfter looking at my main themes, I got curious. What about the `stop_words`? Could they tell a story too?\n\nI started thinking about it when I saw the word “like” so often repeated. I am not sure I use it in my writing as a verb, but more like… well, exactly like that. I would find it interesting to investigate which stop words I use more often, and if I should rethink the way I write.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find and count only the stop words\nstop_words_counted <- entries %>%\n  unnest_tokens(word, entry_text) %>%\n  # Keep only the words that are in the stop_words list\n  inner_join(stop_words, by = c(\"word\" = \"word\")) %>%\n  count(word, sort = TRUE)\n\nstop_words_counted %>% \n  filter(n > 50) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n```\n:::\n\n\n![](img/stop-words.svg)\n\nOk, using the whole stop-words dataset might not have much information. The fact that “I” and \"the\" are the most used ones doesn’t tell me much on its own.\n\nBut I could filter out of this dataset. For example, I started with **modal verbs**. I wanted to see if I was a person of 'shoulds,' 'coulds,' or 'wants.' Then I went over the **pronouns**, then **conjunctions** and **time transition words**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstop_words_counted %>% \n  filter(word %in% c(\"should\", \"could\", \"would\", \"will\", \"want\", \"shall\", \"can\")) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %>% \n  filter(word %in% c(\"i\", \"me\", \"my\", \"myself\", \"you\", \"your\", \"he\", \"she\", \"they\", \"we\")) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %>% \n  filter(word %in% c(\"but\", \"however\", \"although\", \"because\", \"so\", \"therefore\", \"then\")) %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %>% \n  filter(word %in% c(\"before\", \"after\", \"during\", \"since\", \"until\", \"ago\", \"within\"))  %>% \n  mutate(word = reorder(word, n)) %>% \n  ggplot(aes(n,word)) +\n  geom_col()\n```\n:::\n\n\n![](img/1.svg) ![](img/2.svg) ![](img/3.svg) ![](img/4.svg)\n\n## Chapter 5: The road not taken (aka: other methods)\n\nThe [\"Text Mining with R\"](https://www.tidytextmining.com/) book is full of other amazing techniques, and I tried a couple more.\n\nI first tried [`word correlation`](https://www.tidytextmining.com/ngrams.html?q=word%20corre#pairwise-correlation) to see which words I tend to associate with each other. The results were interesting but didn't tell me anything I didn't already expect (yes, `dissertation` is correlated with `paper` or `writing`).\n\nNext, I tried [`topic modeling`](https://www.tidytextmining.com/topicmodeling) to see if an algorithm could automatically find the hidden themes in my journal. This didn't work as well as I'd hoped, likely because my journal entries are often short and context-heavy.\n\nChecking numbers with [`bigram`](http://tidytextmining.com/ngrams.html?q=bigram#analyzing-bigrams) was another interesting topic: I had a lot of numbers in my entries (often coupled with “minutes” or “weeks”, or maybe plain schedules) and I tried to couple numbers with their words, but that didn’t work: I somehow filtered some information from my dataset (the shrews ID from my dissertation have are unique number combinations) so that was a bit of a bummer.\n\nWhile these didn't produce mind-blowing results for my specific dataset, they are incredibly powerful techniques. If you want to learn more, I highly recommend checking out the chapters on word correlation and topic modeling in the \"Tidy Text Mining with R\" book!\n\n### Conclusion: Was it as powerful as I thought?\n\nI approached this project as a beginner, wanting to see if I could find a story in the massive amount of text in my Logseq vault. The mission was biased, of course; I wanted to learn something about myself.\n\nAnd I did. I learned that my focus shifted dramatically during my dissertation, that my mood has a weekly rhythm, and that the tools to find these insights are accessible to anyone willing to learn.\n\nWe all have a massive, unstructured dataset of our own lives in our notes, emails, or calendars. It might look like junk, but it's also a goldmine. The tools to explore it are more accessible than ever, and the only prerequisite is a little bit of curiosity.\n\nSo, is note-taking that powerful? It's powerful enough to turn forgotten thoughts into a map of who you are. And for me, that's powerful enough.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}