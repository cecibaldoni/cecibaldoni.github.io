[
  {
    "objectID": "blog/awesome-pkm.html",
    "href": "blog/awesome-pkm.html",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "",
    "text": "Awesome\n\n\nAt some point during my PhD, I started using Logseq to organize my knowledge.  Then it was Obsidian, then both for a time. Then Logseq again.  Guess what? Now I am back to both.\nLearning to use a note-taking app is an interesting experience. I mean, it‚Äôs something you need to manage your knowledge, but you also need some time to learn how does that actually work. I guess if you already have a great note-taking routine (I hadn‚Äôt), you know how to structure your thoughts (carnival music roaming) and you have some basic idea on how to connect your ideas (lucky you) then the learning curve is reasonable.\nFor me it was indeed interesting. I downloaded Logseq with no ideas about how to make it work. My very first entry (September 15th, 2021) recites:\n\nI installed this thing. Not sure how it works (yet). \ncit. me as a confused Logseq first-timer\n\nTruer words have never been said. That was all for that week, no more entries.  But then it was like a videogame. You keep thinking about it and you keep trying.  At some point you reach some level of certainty and move forward.  Each new level brought its own set of complexities. The longer I used it, the more questions popped up: how do people structure their vaults? What‚Äôs a good way to handle academic reading? Is there a Zotero workflow that won‚Äôt break my brain?\nIf you are here because you‚Äôd like an answer to these questions, I apologize in advance.  I do not think there is anything like this in the whole wide web. That‚Äôs because it depends on you. When I gave a workshop to PhD students about note-taking apps and PKM I had a mantra (I think at some point I should have written it on a t-shirt, I was saying it every three or four sentences):\n\nWhat works for me might not work for you. \ncit. me as a recalcitrant Logseq user\n\nI have my own workflow. I can show it to you and you can try it, but I bet you will find something about it that it‚Äôs just not right.  But then again, the internet if full of people finding their own workflow and sharing it with others (bless them).\nThat‚Äôs when I realized: I had quietly been collecting a pretty large pile of resources that other researchers (and future me!) might find useful.\nIt took a while to organize everything, but eventually I decided it was worth turning into a proper list.\nüëâ Awesome PKM for Academics and Researchers"
  },
  {
    "objectID": "blog/awesome-pkm.html#a-journey-often-starts-with-chaos-and-confusion",
    "href": "blog/awesome-pkm.html#a-journey-often-starts-with-chaos-and-confusion",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "",
    "text": "Awesome\n\n\nAt some point during my PhD, I started using Logseq to organize my knowledge.  Then it was Obsidian, then both for a time. Then Logseq again.  Guess what? Now I am back to both.\nLearning to use a note-taking app is an interesting experience. I mean, it‚Äôs something you need to manage your knowledge, but you also need some time to learn how does that actually work. I guess if you already have a great note-taking routine (I hadn‚Äôt), you know how to structure your thoughts (carnival music roaming) and you have some basic idea on how to connect your ideas (lucky you) then the learning curve is reasonable.\nFor me it was indeed interesting. I downloaded Logseq with no ideas about how to make it work. My very first entry (September 15th, 2021) recites:\n\nI installed this thing. Not sure how it works (yet). \ncit. me as a confused Logseq first-timer\n\nTruer words have never been said. That was all for that week, no more entries.  But then it was like a videogame. You keep thinking about it and you keep trying.  At some point you reach some level of certainty and move forward.  Each new level brought its own set of complexities. The longer I used it, the more questions popped up: how do people structure their vaults? What‚Äôs a good way to handle academic reading? Is there a Zotero workflow that won‚Äôt break my brain?\nIf you are here because you‚Äôd like an answer to these questions, I apologize in advance.  I do not think there is anything like this in the whole wide web. That‚Äôs because it depends on you. When I gave a workshop to PhD students about note-taking apps and PKM I had a mantra (I think at some point I should have written it on a t-shirt, I was saying it every three or four sentences):\n\nWhat works for me might not work for you. \ncit. me as a recalcitrant Logseq user\n\nI have my own workflow. I can show it to you and you can try it, but I bet you will find something about it that it‚Äôs just not right.  But then again, the internet if full of people finding their own workflow and sharing it with others (bless them).\nThat‚Äôs when I realized: I had quietly been collecting a pretty large pile of resources that other researchers (and future me!) might find useful.\nIt took a while to organize everything, but eventually I decided it was worth turning into a proper list.\nüëâ Awesome PKM for Academics and Researchers"
  },
  {
    "objectID": "blog/awesome-pkm.html#disclaimer.",
    "href": "blog/awesome-pkm.html#disclaimer.",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "Disclaimer.",
    "text": "Disclaimer.\nThere are already ‚Äúawesome PKM‚Äù lists out there, a quick github search will show you 5 others. And they are great.  But somehow some are pretty general, or focused on the tools, not the context in which people use them.\nThis list is my attempt at something more specific: a curated (and opinionated) resource library, with researchers and PhD students in mind."
  },
  {
    "objectID": "blog/awesome-pkm.html#whats-inside",
    "href": "blog/awesome-pkm.html#whats-inside",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "What‚Äôs Inside",
    "text": "What‚Äôs Inside\nIt includes:\n\nForum threads where people talk about how they actually use PKM tools in academia.\nBlog posts about real workflows; e.g.¬†Zotero to Obsidian, or how to organize a literature review in Logseq.\nReady-to-use vaults and templates (some very polished, some simple but helpful).\nVideos and talks that show how people think through their own systems (these are one of my personal favourite. I could listen for hours to people on youtube telling me how they structure their academic knowledge).\nAcademic papers on knowledge graphs, note-taking, and scholarly PKM.\n\nI‚Äôve grouped everything into categories, so it‚Äôs easy to browse or come back to later."
  },
  {
    "objectID": "blog/awesome-pkm.html#who-this-is-for",
    "href": "blog/awesome-pkm.html#who-this-is-for",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "Who This Is For",
    "text": "Who This Is For\nThis might be useful if you:\n\nare writing a thesis (Bachelor, Master, Doctorate) and losing track of all your notes\nwant to stop reinventing your reading workflow every semester (been there, done that)\ntried using Obsidian or Logseq and didn‚Äôt know where to start\nfeel like your brain is full of articles you never quite remember at the right time\nyou have a to-read list that is longer than your 12yo Christmas wishlist\n\nBasically: if you‚Äôre a researcher trying to manage knowledge better, this list is for you."
  },
  {
    "objectID": "blog/awesome-pkm.html#want-to-add-something",
    "href": "blog/awesome-pkm.html#want-to-add-something",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "Want to Add Something?",
    "text": "Want to Add Something?\nPlease do! I‚Äôd love to keep this evolving. You can:\n\nsubmit a pull request\nopen an issue\nor just message me if you‚Äôre unsure how to do those and still have something you would like to add.\n\nTips, workflows, example vaults, tiny-but-useful plugins‚Ä¶ anything that helps researchers use PKM more intentionally is welcome."
  },
  {
    "objectID": "blog/awesome-pkm.html#tldr",
    "href": "blog/awesome-pkm.html#tldr",
    "title": "From ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù",
    "section": "TL;DR",
    "text": "TL;DR\nI just wanted a place to put all the good stuff in, and maybe help a few other people skip the part where they re-Google ‚ÄúZotero to Obsidian academic workflow‚Äù every three weeks.\nHere‚Äôs the repo: üëâ Awesome PKM for Academics and Researchers"
  },
  {
    "objectID": "blog/closeread-cheatsheet.html",
    "href": "blog/closeread-cheatsheet.html",
    "title": "A Cheatsheet for Closeread (or the one where I gave a talk and brought more than slides)",
    "section": "",
    "text": "So, I got invited to give a talk for R-Ladies Amsterdam. About something I really, really like: scrollytelling with Quarto and Closeread.\nThe invitation came after I shared a peek at my Closeread-built research page online. The post picked up a bit of attention. Apparently, the combination of tiny shrews and scroll animations is memorable (take notes, academia!). Slowly it turned from what it showed to how I made it. That eventually turned into: ‚ÄúWould you like to give a talk about this?‚Äù.\nBy the way, I said yes to the talk. You already know that part, you‚Äôre here reading the aftermath, afterall.\nI love talks like this. I get to share something I‚Äôm genuinely excited about, with people who might also be curious. And I get to say: ‚ÄúHey, this is easier than it looks. And also fun!‚Äù\nWhen I started getting things ready, I thought I‚Äôd just need some slides. Maybe a live demo. But then came the custom SCSS file. And the example repo. And somewhere along the way, a cheatsheet happened too.\nIt wasn‚Äôt really part of the plan. But I kept reopening the documentation to check things, and at some point I realized I was having the same three doubts on repeat. Eventually, I gave up on flipping between tabs and just made a cheatsheet. It was half an act of self-care, half a design itch.\n\n\n\nA cheatsheet for Closeread\n\n\nThe result is a printable cheatsheet that puts the main Closeread building blocks in one spot. It‚Äôs colorful (because why not), and it‚Äôs practical (because I needed it), and if you‚Äôre starting your own Closeread experiment, I hope it saves you a few tabs.\nI wrapped everything into a tidy GitHub repo; you will find the slides from the talk, the closeread example folder, and the cheatsheet. You can fork it, remix it, or ignore it completely. But if you‚Äôre curious, it‚Äôs there.\nAs for the talk‚Ä¶ I think it went well. You can watch it here if you are curious. I was very excited (is too excited possible?). Also, mildly terrified for the classic Zoom shenanigans: where is the link, screen share panic, going too fast and suddenly realizing I‚Äôm done five minutes early. But mostly, I‚Äôm just happy this tiny visual storytelling project has found its way into other people‚Äôs hands.\nIf you want to try Closeread, everything‚Äôs in the repo. And if you want to look at shrews with scroll-activated flair, well‚Ä¶ I support that too.\nHappy scrolling."
  },
  {
    "objectID": "blog/closeread-cheatsheet.html#how-i-ended-up-making-a-closeread-cheatsheet-and-a-talk",
    "href": "blog/closeread-cheatsheet.html#how-i-ended-up-making-a-closeread-cheatsheet-and-a-talk",
    "title": "A Cheatsheet for Closeread (or the one where I gave a talk and brought more than slides)",
    "section": "",
    "text": "So, I got invited to give a talk for R-Ladies Amsterdam. About something I really, really like: scrollytelling with Quarto and Closeread.\nThe invitation came after I shared a peek at my Closeread-built research page online. The post picked up a bit of attention. Apparently, the combination of tiny shrews and scroll animations is memorable (take notes, academia!). Slowly it turned from what it showed to how I made it. That eventually turned into: ‚ÄúWould you like to give a talk about this?‚Äù.\nBy the way, I said yes to the talk. You already know that part, you‚Äôre here reading the aftermath, afterall.\nI love talks like this. I get to share something I‚Äôm genuinely excited about, with people who might also be curious. And I get to say: ‚ÄúHey, this is easier than it looks. And also fun!‚Äù\nWhen I started getting things ready, I thought I‚Äôd just need some slides. Maybe a live demo. But then came the custom SCSS file. And the example repo. And somewhere along the way, a cheatsheet happened too.\nIt wasn‚Äôt really part of the plan. But I kept reopening the documentation to check things, and at some point I realized I was having the same three doubts on repeat. Eventually, I gave up on flipping between tabs and just made a cheatsheet. It was half an act of self-care, half a design itch.\n\n\n\nA cheatsheet for Closeread\n\n\nThe result is a printable cheatsheet that puts the main Closeread building blocks in one spot. It‚Äôs colorful (because why not), and it‚Äôs practical (because I needed it), and if you‚Äôre starting your own Closeread experiment, I hope it saves you a few tabs.\nI wrapped everything into a tidy GitHub repo; you will find the slides from the talk, the closeread example folder, and the cheatsheet. You can fork it, remix it, or ignore it completely. But if you‚Äôre curious, it‚Äôs there.\nAs for the talk‚Ä¶ I think it went well. You can watch it here if you are curious. I was very excited (is too excited possible?). Also, mildly terrified for the classic Zoom shenanigans: where is the link, screen share panic, going too fast and suddenly realizing I‚Äôm done five minutes early. But mostly, I‚Äôm just happy this tiny visual storytelling project has found its way into other people‚Äôs hands.\nIf you want to try Closeread, everything‚Äôs in the repo. And if you want to look at shrews with scroll-activated flair, well‚Ä¶ I support that too.\nHappy scrolling."
  },
  {
    "objectID": "blog/4rs.html",
    "href": "blog/4rs.html",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "What do we actually mean when we say something is reproducible? Or replicable? Or reliable? These words come up all the time, especially when you are reading about open science or trying to make your research more transparent. They sound straightforward, but we do not always use them the same way, even when we think we do.\nA few months ago, while preparing a talk on open science (you can find the presentation here), I ran into this tangle: I had reached the part about the four Rs, and, in my head, things were a bit confused. I did what I often do when something does not quite make sense: I made a visual. In this case, I wanted to map out reliable, repeatable, reproducible, and replicable, the way I had learned them. It was not meant to be a final word on anything. I just needed a clearer picture for myself.\n\nIn the infographic, I defined them like this:\n\nReliable: when you use the same data and the same analyses and you get the same results\n\nRepeatable: when you use new data but the same analysis and you get the same results\n\nReplicable: when someone else uses new data and the same analysis and they get the same results\n\nReproducible: when someone else uses the same data and the same analyses and they get the same results\n\nIn other words, these terms all ask a similar question: if we try the same process again, either with the same ingredients or some new ones, do we get the same outcome? The differences lie in who is doing the work, what data they are using, and whether the analysis method stays the same.\n\n\n\nA few weeks later, I attended a great workshop hosted by SORTEE, the Society for Open, Reliable, and Transparent Ecology and Evolution. It focused on reproducible data analysis in R, and after the main talk, someone posted a question in the community Slack: ‚Äúwhat exactly is the difference between reproducible and reliable?‚Äù\nI thought it was a simple follow-up question, something we would all agree on quickly. Several people responded, and as the conversation went on, it became clear we were not saying quite the same thing. Each of us had a slightly different take on what the terms meant (and none of us were strangers to the topic!).\nIt turns out that agreeing on being rigorous is easier than agreeing on what the words mean.\n\n\n\nSo, who was right in that thread? It turns out, it depends who you ask.\nAfter looking into it more carefully, I realised that the same two terms, reproducibility and replicability, are used across many fields, but people do not always mean the same thing by them.\nIf you are working in computational science, like data science or software development, reproducibility usually means re-running someone else‚Äôs code with the same data and getting the same output. It is about whether your pipeline works and whether it can be used again by someone else. Replicability, on the other hand, is when someone collects new data, applies the same method, and gets similar results. The focus here is often on reproducible code, or documented workflows.\nIn psychology or the life sciences, things look a bit different. Reproducibility tends to mean reanalyzing the same dataset and seeing if you come to the same statistical conclusions. Replicability usually means repeating the study with new participants, or samples, or data, and checking whether the same effect appears again. The focus is less on rerunning code and more on checking whether findings hold up when repeated.\nIn the social sciences, there is even more variation. Replicability often includes testing whether a finding holds in a new context, such as a different population, setting, or time. Reproducibility still involves using the same data and methods, but with more emphasis on sharing not just data, but also things like surveys, instruments, and procedures. The big question here is whether the result still holds when we move it somewhere else.\nThe National Academies of Sciences tried to create a shared baseline across disciplines in their 2019 report. Their definitions are:\n- Reproducibility is getting the same results using the same data and the same analysis\n- Replicability is getting consistent results when a study is repeated with new data and the same methods\nThe Turing Way, an open-source community guide to reproducible, ethical, and collaborative data science, stays close to the computational science view. It defines reproducibility as obtaining the same results when the same analysis steps are performed on the same dataset, and replicability as obtaining qualitatively similar results when the same analysis is applied to a different dataset.\n\nNote: to be fair, The Turing Way also introduces additional concepts like robustness and generalisability, but I will not go into those here, or I will lose the thin thread of my thought. If you are interested in following up, read them here!\nSo while the words are shared, the meaning behind them shifts depending on the field. Sometimes the question is whether something runs again. Sometimes it is whether it works again. And sometimes it is whether it still matters in a new setting.\nThat is where most of the confusion starts. And, in a way, that is also where the interesting conversations begin.\n\n\n\nDo I still like it? Yes. It helped me understand the conceptual space, and it got a useful conversation going. In some fields, the definitions I used match common usage. In others, they might not. And that is okay. For what it is worth, my version is very close to how the National Academies of Sciences define the terms. But context always matters more than labels.\nWhat I have learned is that it is less important to be right than to recognise that the definitions vary. If you are using one of these terms, define it, even briefly. This can go a long way in avoiding misunderstandings, especially when you are working across disciplines.\n\n\n\nThis whole experience, making a diagram, joining a discussion, watching it unravel slightly, and learning from the mess, left me with a few reflections:\n\nDefinitions are not set in stone. They evolve, and that is something open science helps us see more clearly. When we work in transparent, collaborative ways, we start noticing the gaps and overlaps in how we talk about things. Where you are, what you are working on, and who you are working with can all shape what these terms mean to you\n\nWhen you use a term like reproducible or replicable, take a second to say what you mean by it\n\nVisuals can be powerful tools for thinking and communicating, even if they do not capture every nuance\n\nIf you are confused by these terms, you are not alone. I gave a whole talk on the topic and still walked away with more questions than answers, but also a clearer sense of what to ask next. That feels like progress to me\n\n\n\n\nI made an infographic.\nI joined a discussion.\nThe discussion got a bit messy.\nI learned a lot.\nOpen science is not about always getting things right the first time. It is about being transparent enough to revise your thinking, share what you have learned, and leave room for complexity. This post is part of that process.\nFor now, I will leave the diagram as it is. Maybe I will revise it later. Either way, I hope it gets people thinking, and talking."
  },
  {
    "objectID": "blog/4rs.html#trying-to-make-sense-of-the-four-rs",
    "href": "blog/4rs.html#trying-to-make-sense-of-the-four-rs",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "What do we actually mean when we say something is reproducible? Or replicable? Or reliable? These words come up all the time, especially when you are reading about open science or trying to make your research more transparent. They sound straightforward, but we do not always use them the same way, even when we think we do.\nA few months ago, while preparing a talk on open science (you can find the presentation here), I ran into this tangle: I had reached the part about the four Rs, and, in my head, things were a bit confused. I did what I often do when something does not quite make sense: I made a visual. In this case, I wanted to map out reliable, repeatable, reproducible, and replicable, the way I had learned them. It was not meant to be a final word on anything. I just needed a clearer picture for myself.\n\nIn the infographic, I defined them like this:\n\nReliable: when you use the same data and the same analyses and you get the same results\n\nRepeatable: when you use new data but the same analysis and you get the same results\n\nReplicable: when someone else uses new data and the same analysis and they get the same results\n\nReproducible: when someone else uses the same data and the same analyses and they get the same results\n\nIn other words, these terms all ask a similar question: if we try the same process again, either with the same ingredients or some new ones, do we get the same outcome? The differences lie in who is doing the work, what data they are using, and whether the analysis method stays the same."
  },
  {
    "objectID": "blog/4rs.html#the-moment-things-got-interesting",
    "href": "blog/4rs.html#the-moment-things-got-interesting",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "A few weeks later, I attended a great workshop hosted by SORTEE, the Society for Open, Reliable, and Transparent Ecology and Evolution. It focused on reproducible data analysis in R, and after the main talk, someone posted a question in the community Slack: ‚Äúwhat exactly is the difference between reproducible and reliable?‚Äù\nI thought it was a simple follow-up question, something we would all agree on quickly. Several people responded, and as the conversation went on, it became clear we were not saying quite the same thing. Each of us had a slightly different take on what the terms meant (and none of us were strangers to the topic!).\nIt turns out that agreeing on being rigorous is easier than agreeing on what the words mean."
  },
  {
    "objectID": "blog/4rs.html#same-terms-different-fields",
    "href": "blog/4rs.html#same-terms-different-fields",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "So, who was right in that thread? It turns out, it depends who you ask.\nAfter looking into it more carefully, I realised that the same two terms, reproducibility and replicability, are used across many fields, but people do not always mean the same thing by them.\nIf you are working in computational science, like data science or software development, reproducibility usually means re-running someone else‚Äôs code with the same data and getting the same output. It is about whether your pipeline works and whether it can be used again by someone else. Replicability, on the other hand, is when someone collects new data, applies the same method, and gets similar results. The focus here is often on reproducible code, or documented workflows.\nIn psychology or the life sciences, things look a bit different. Reproducibility tends to mean reanalyzing the same dataset and seeing if you come to the same statistical conclusions. Replicability usually means repeating the study with new participants, or samples, or data, and checking whether the same effect appears again. The focus is less on rerunning code and more on checking whether findings hold up when repeated.\nIn the social sciences, there is even more variation. Replicability often includes testing whether a finding holds in a new context, such as a different population, setting, or time. Reproducibility still involves using the same data and methods, but with more emphasis on sharing not just data, but also things like surveys, instruments, and procedures. The big question here is whether the result still holds when we move it somewhere else.\nThe National Academies of Sciences tried to create a shared baseline across disciplines in their 2019 report. Their definitions are:\n- Reproducibility is getting the same results using the same data and the same analysis\n- Replicability is getting consistent results when a study is repeated with new data and the same methods\nThe Turing Way, an open-source community guide to reproducible, ethical, and collaborative data science, stays close to the computational science view. It defines reproducibility as obtaining the same results when the same analysis steps are performed on the same dataset, and replicability as obtaining qualitatively similar results when the same analysis is applied to a different dataset.\n\nNote: to be fair, The Turing Way also introduces additional concepts like robustness and generalisability, but I will not go into those here, or I will lose the thin thread of my thought. If you are interested in following up, read them here!\nSo while the words are shared, the meaning behind them shifts depending on the field. Sometimes the question is whether something runs again. Sometimes it is whether it works again. And sometimes it is whether it still matters in a new setting.\nThat is where most of the confusion starts. And, in a way, that is also where the interesting conversations begin."
  },
  {
    "objectID": "blog/4rs.html#what-about-the-infographic",
    "href": "blog/4rs.html#what-about-the-infographic",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "Do I still like it? Yes. It helped me understand the conceptual space, and it got a useful conversation going. In some fields, the definitions I used match common usage. In others, they might not. And that is okay. For what it is worth, my version is very close to how the National Academies of Sciences define the terms. But context always matters more than labels.\nWhat I have learned is that it is less important to be right than to recognise that the definitions vary. If you are using one of these terms, define it, even briefly. This can go a long way in avoiding misunderstandings, especially when you are working across disciplines."
  },
  {
    "objectID": "blog/4rs.html#a-few-things-i-took-away",
    "href": "blog/4rs.html#a-few-things-i-took-away",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "This whole experience, making a diagram, joining a discussion, watching it unravel slightly, and learning from the mess, left me with a few reflections:\n\nDefinitions are not set in stone. They evolve, and that is something open science helps us see more clearly. When we work in transparent, collaborative ways, we start noticing the gaps and overlaps in how we talk about things. Where you are, what you are working on, and who you are working with can all shape what these terms mean to you\n\nWhen you use a term like reproducible or replicable, take a second to say what you mean by it\n\nVisuals can be powerful tools for thinking and communicating, even if they do not capture every nuance\n\nIf you are confused by these terms, you are not alone. I gave a whole talk on the topic and still walked away with more questions than answers, but also a clearer sense of what to ask next. That feels like progress to me"
  },
  {
    "objectID": "blog/4rs.html#tldr",
    "href": "blog/4rs.html#tldr",
    "title": "The 4 Rs, and why they‚Äôre so confusing",
    "section": "",
    "text": "I made an infographic.\nI joined a discussion.\nThe discussion got a bit messy.\nI learned a lot.\nOpen science is not about always getting things right the first time. It is about being transparent enough to revise your thinking, share what you have learned, and leave room for complexity. This post is part of that process.\nFor now, I will leave the diagram as it is. Maybe I will revise it later. Either way, I hope it gets people thinking, and talking."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "So, you want to start Sketchnoting (but don‚Äôt know how)\n\n\n\nnote-taking\n\nsketchnoting\n\n\n\nThis post is for you if you‚Äôve ever looked at a sketchnote and thought: ‚ÄúThat‚Äôs cool, but I could never do that.‚Äù\n\n\n\n\n\nSep 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIs Note-Taking as powerful as I think? (2/2)\n\n\n\nPKM\n\nnote-taking\n\n\n\nPart 2 of 2 - This post shows how I dived into my digital journal to extract information about my writing style, my weekly mood, and yes, make more plots.\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNote-Taking is more powerful than you think (1/2)\n\n\n\nPKM\n\nnote-taking\n\n\n\nPart 1 of 2 - This post shows how I transformed simple daily work logs in Logseq into a powerful data visualization in R to better understand my own strengths and interests.\n\n\n\n\n\nAug 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday and Quartose\n\n\n\nTidyTuesday\n\nquarto\n\nquartose\n\n\n\nAn experiment in structuring a TidyTuesday exploration using interactive elements from quartose\n\n\n\n\n\nJul 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom ‚ÄúNot Sure How It Works‚Äù to ‚ÄúMaybe This Will Help Someone‚Äù\n\n\n\nPKM\n\nnote-taking\n\nopen resources\n\n\n\nThis is the story on how I went from Confused Logseq User to Accidental Curator of an awesome academic PKM repository\n\n\n\n\n\nJun 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Cheatsheet for Closeread (or the one where I gave a talk and brought more than slides)\n\n\n\nquarto\n\ncloseread\n\ninfographic\n\nopen resources\n\n\n\nThis post started with a talk invitation about Closeread and scrollytelling. It ended with a full demo and a printable cheatsheet, all thanks to aesthetic joy and the refusal to keep checking the same three lines of syntax.\n\n\n\n\n\nMay 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe 4 Rs, and why they‚Äôre so confusing\n\n\n\nopen science\n\ninfographic\n\n\n\nThis post started with a diagram I made for a talk, and ended with a discussion that showed me just how slippery scientific terms can be.\n\n\n\n\n\nMay 8, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/privacy.html",
    "href": "content/privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "This website uses Google Analytics to help understand how visitors interact with the content.\nNo personal data (like names or emails) is collected directly. Google Analytics may use cookies to collect anonymous usage data such as page views, duration, and device/browser information.\nBy using this site, you consent to this anonymous tracking.\nYou can disable cookies in your browser settings or use browser extensions to block analytics scripts.\nIf you have questions, feel free to contact me."
  },
  {
    "objectID": "content/about.html#about-me",
    "href": "content/about.html#about-me",
    "title": "Cecilia Baldoni",
    "section": "About me",
    "text": "About me\n\n  \n  \n    \n      Scientist\n      Enjoy designing and leading workshops\n      Love connecting research ideas with everyday challenges\n      Obsessed with note-taking apps\n      Always curious, always learning something new\n\n  \n\n I work at the intersection of science, communication, and collaboration.\nMy current work centers on the cognitive compromises of the common shrew, a model species for studying brain size plasticity and behavior.\nI combine research with hands-on leadership to build communities that empower learning and collaboration. I‚Äôm passionate about facilitating workshops, supporting digital collaboration, and I focus on making complex ideas accessible and connecting people across fields. You can see examples of this in my Talks & Workshops and Projects pages.\nI am deeply involved in open science practices, and I aim to advance openness, transparency and reproducibility in my daily life as a researcher. I believe openness is both a technical choice and cultural one, about how we share knowledge and invite others to build on it.\nHere are a few highlights of my community impact so far:\n\nElected representative during my PhD and postdoc, giving voice to peers across departments\nLeading the SORTEE code club and helping run the R-Ladies blog, nurturing inclusive spaces for data science\nPart of The Turing Way‚Äôs community management working group, shaping best practices for open collaboration\nTranslator for the Raspberry Pi Foundation and code editor for PCI Ecology; helping global audiences access science and coding content\nWorkshop leader for R programming and personal knowledge management, demystifying tools to help others level up\n\nI‚Äôm always curious about new ways to connect people, ideas, and tools‚Äîand I genuinely enjoy helping others learn and grow.\nIf you want to collaborate, share ideas, or just have a friendly chat, feel free to reach out or book some time to talk."
  },
  {
    "objectID": "content/project.html#current-research",
    "href": "content/project.html#current-research",
    "title": "Cecilia Baldoni",
    "section": "Projects",
    "text": "Projects\n\n\n\n  \n    \n    Shrews\n    Seasonal brain plasticity, cognition, shrews... explored through data and visual storytelling.\n  \n\n\n\n  \n    \n    Workshops & Talks\n    Conferences, seminars, and hands-on workshops on open workflows and data science.\n  \n\n\n\n  \n    \n    Open Science\n    Personal and community efforts to improve transparency and reproducibility in science.\n  \n\n\n\n  \n    \n    Datasets\n    Published and openly available datasets from my research.\n  \n\n\n\n  \n    \n    Illustrations\n    Science-inspired art and illustrations.\n  \n\n\n\n  \n    \n    Publications\n    My academic (and not) publications."
  },
  {
    "objectID": "content/illustrations.html",
    "href": "content/illustrations.html",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "In this page you can find a selection of illustrations, sketches, and visual experiments I‚Äôve created for different projects.  I love combining science and design, and helping others visually communicate their work.  You can support my work or request a custom commission (graphical abstracts, favicons, logos, or visuals for websites and talks) through Ko-fi:\n\n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n\nThese are elements from the illustrated isometric city I designed for my homepage. Each building represents a different section of my site ‚Äî and yes, I drew all of them!\n\n  \n  \n\n ¬© cecibaldoni, 2025. All works in this page are licensed under a Creative Commons Attribution-NonDerivative-NonCommercial 4.0 International License.  This means you‚Äôre welcome to share anything from this page for non-commercial purposes as long as you credit me as the creator!\n\n\nWant a high-res version or a custom illustration? Contact me hereÔ∏è!\n\n√ó"
  },
  {
    "objectID": "content/illustrations.html#illustrations",
    "href": "content/illustrations.html#illustrations",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "In this page you can find a selection of illustrations, sketches, and visual experiments I‚Äôve created for different projects.  I love combining science and design, and helping others visually communicate their work.  You can support my work or request a custom commission (graphical abstracts, favicons, logos, or visuals for websites and talks) through Ko-fi:\n\n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n\n\n\n  \n  \n  \n  \n  \n  \n\nThese are elements from the illustrated isometric city I designed for my homepage. Each building represents a different section of my site ‚Äî and yes, I drew all of them!\n\n  \n  \n\n ¬© cecibaldoni, 2025. All works in this page are licensed under a Creative Commons Attribution-NonDerivative-NonCommercial 4.0 International License.  This means you‚Äôre welcome to share anything from this page for non-commercial purposes as long as you credit me as the creator!\n\n\nWant a high-res version or a custom illustration? Contact me hereÔ∏è!\n\n√ó"
  },
  {
    "objectID": "content/shrews.html",
    "href": "content/shrews.html",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "I study the common shrew (Sorex araneus), a tiny mammal with a metabolism so extreme that even a few hours without food can be fatal.\n\n\n\n\nThis high metabolism makes them particularly vulnerable to seasonal changes, and it makes it impossible for them to hibernate during winter. Their solution? They shrink.\n\n\n\n\nUnlike animals that grow steadily over years, shrews undergo a radical transformation within a single lifetime.\n\n\n\n\nThis dramatic change, Dehnel‚Äôs Phenomenon, reduces their brain, bones, and organs to save energy in winter scarcity, regrowing them in spring for breeding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\nBorn in summer, shrews rapidly reach peak brain size. This is the largest their brains will ever be.\n\n\n\n\nAs winter sets in, the shrew shrinks their brains and bodies, reducing energy needs.\n\n\n\n\nBy spring, they double their winter mass, gearing up to reproduce.\n\n\n\n\nShrews live only about a year, so this transformation happens just once.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens cognitively when brains shrink and regrow?\n\n\n\n\nMy research explores what happens when their brain changes size:\n\n\n\n\nHow does brain size affect cognitive abilities?\n\n\n\n\nDoes a smaller brain lead to simpler decision-making?\n\n\n\n\nThese questions go beyond just shrews. Understanding how an animal maintains function despite physical changes in the brain helps us explore broader ideas in neurodegeneration and energy efficiency."
  },
  {
    "objectID": "content/shrews.html#shrew",
    "href": "content/shrews.html#shrew",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "I study the common shrew (Sorex araneus), a tiny mammal with a metabolism so extreme that even a few hours without food can be fatal.\n\n\n\n\nThis high metabolism makes them particularly vulnerable to seasonal changes, and it makes it impossible for them to hibernate during winter. Their solution? They shrink.\n\n\n\n\nUnlike animals that grow steadily over years, shrews undergo a radical transformation within a single lifetime.\n\n\n\n\nThis dramatic change, Dehnel‚Äôs Phenomenon, reduces their brain, bones, and organs to save energy in winter scarcity, regrowing them in spring for breeding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\nBorn in summer, shrews rapidly reach peak brain size. This is the largest their brains will ever be.\n\n\n\n\nAs winter sets in, the shrew shrinks their brains and bodies, reducing energy needs.\n\n\n\n\nBy spring, they double their winter mass, gearing up to reproduce.\n\n\n\n\nShrews live only about a year, so this transformation happens just once.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens cognitively when brains shrink and regrow?\n\n\n\n\nMy research explores what happens when their brain changes size:\n\n\n\n\nHow does brain size affect cognitive abilities?\n\n\n\n\nDoes a smaller brain lead to simpler decision-making?\n\n\n\n\nThese questions go beyond just shrews. Understanding how an animal maintains function despite physical changes in the brain helps us explore broader ideas in neurodegeneration and energy efficiency."
  },
  {
    "objectID": "content/shrews.html#shrew-captivity",
    "href": "content/shrews.html#shrew-captivity",
    "title": "Cecilia Baldoni",
    "section": "Does captivity influence learning?",
    "text": "Does captivity influence learning?\n\n\n\n\nBrain size is of course only part of the story‚Ä¶ environment shapes behavior too.\n\n\n\n\nIn captivity, animals face new challenges. Do shrews kept in captivity learn differently than those in the wild?\n\n\n\n\nI tested the ability of shrews to associate an odor cue with a reward in a Y-maze associative learning task.\n\n\n\n\nEach dot represents an individual shrew‚Äôs decision in a trial. The x-axis represents the trial number, from 1 to 10. The y-axis shows whether they made the correct choice (1) or the wrong one (0).\n\n\n\n\nSummer wild (orange) shrews had the highest and earliest success rates.\n\n\n\n\nWinter wild (dark blue) shrews showed a similar learning curve, but they learned at a slower rate and did not reach the Summer success rate.\n\n\n\n\nWinter captive (light blue) shrews showed no learning, never deviating from chance.\n\n\n\n\nüìÑ This work is published in Royal Society Open Science: Captivity alters behaviour but not seasonal brain size change in semi-naturally housed shrews\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Collaborators\nI conduct this research supervised by Dr.¬†Dina Dechmann at the Max Planck Institute of Animal Behaviour\n Collaborators:  Prof.¬†Dominik von Elverfeldt, University of Freiburg, Germany  Prof.¬†Liliana D√°valos, Stony Brook University, New York  Prof.¬†John Nieland, University of Aalborg, Denmark"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Welcome to my website!  I am an Early Career Researcher at the Max Planck Institute of Animal Behaviour where I explore how the brain and environment shape adaptable minds.\nAlongside my work, I‚Äôm dedicated to creating spaces where science and community come together to share knowledge and spark collaboration.\nDive into the interactive city below to explore my projects, workshops, open science efforts, and community activities. \n\n\n\n  \n    \n\n    \n      \n      Get in Touch!\n    \n\n    \n      \n      About Me\n    \n\n    \n      \n      Illustrations\n    \n\n    \n      \n      Open Science\n    \n\n    \n      \n      Research Interests\n    \n\n    \n      \n      Talks and Workshops\n    \n    \n    \n      \n      Blog"
  },
  {
    "objectID": "index.html#explore-the-city",
    "href": "index.html#explore-the-city",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Get in Touch!\n    \n\n    \n      \n      About Me\n    \n\n    \n      \n      Illustrations\n    \n\n    \n      \n      Open Science\n    \n\n    \n      \n      Research Interests\n    \n\n    \n      \n      Talks and Workshops\n    \n    \n    \n      \n      Blog"
  },
  {
    "objectID": "content/datasets.html",
    "href": "content/datasets.html",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "content/datasets.html#datasets",
    "href": "content/datasets.html#datasets",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "content/talks.html#talks-workshops",
    "href": "content/talks.html#talks-workshops",
    "title": "Cecilia Baldoni",
    "section": "Workshops and Talks",
    "text": "Workshops and Talks\n\nWorkshops\n\nData wrangling with {tidyverse}\nEvent link ¬∑ Slides\nScrollytelling Science with Quarto and Closeread Youtube ¬∑ Slides\nWhat is Open Science Anyway?\nEvent link ¬∑ Slides\nSORTEE Code Club\nList of events\nPersonal Knowledge Management and Note-Taking Apps\nOpen Resources\n\n\n\n\nConference Talks & Seminars\n\n2025\n\nBaldoni C.\nBeyond the standard: Unconventional vertebrate models in biomedicine EMBO, Edinburg, UK\nThe shrew as a model for brain shrinkage without neurodegeneration\nBaldoni C. Invited Seminar ‚Äî Messerli Institute, Vienna, Austria\nShrinking Shrews: Cognitive Challenges in a Changing Brain\n\n\n\n2024\n\nBaldoni C., Farantouri M., Raptis K., Nourani E., Dechmann D.\nInternational Society for Behavioral Ecology (ISBE), Melbourne, Australia\nSeasonal Brain Size Variation and Its Impact on Spatial Navigation and Learning in the Common Shrew (Sorex araneus)\n\n\n\n2023\n\nBaldoni C., von Elverfeldt D., Dechmann D.\nMPIAB RADO Seminar, Radolfzell, Germany\nNews from the Shrews: Brain structure during Dehnel‚Äôs phenomenon from neuroimaging\nBaldoni C., Farantouri M., Raptis K., Nourani E., Dechmann D.\nAnimal Behaviour Conference, Bielefeld, Germany\nBrain size and spatial learning in the common shrew (Sorex araneus)\nBaldoni C., Nourani E., Dechmann D.\nIV Convegno Nazionale sui Piccoli Mammiferi, Grosseto, Italy\nSeasonal brain size changes affect spatial learning in common shrews (Sorex araneus)\n\n\n\n2022\n\nBaldoni C., Dechmann D.\nAnimal Behaviour Society, San Jos√©, Costa Rica\nSeasonal brain changes impact associative learning in common shrews (Sorex araneus)\n\n\n\n2019\n\nBaldoni C., Scaravelli D., et al.\nIV Italian Bat Conference, Padova, Italy\nNocturnal bat migration at the bird ringing station ‚ÄòBocca di Caset‚Äô\nBaldoni C., Cicero M., et al.\nXX Italian Ornithology Conference, Naples, Italy\nCitizen-science approach to Ring-necked parakeet distribution in Bologna\nBaldoni C., Brucks D., et al.\nASAB Summer Conference, Konstanz, Germany\nSelf-control in two macaw species using a rotating paradigm\n\n\n\n2018\n\nBaldoni C., Giglio G., Scaravelli D.\nII European Meeting of Young Ornithologists, Torino, Italy\nEffects of fire on birds in Mediterranean woods"
  },
  {
    "objectID": "content/publications.html",
    "href": "content/publications.html",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Programmed seasonal brain shrinkage in the common shrew via water loss without cell death (2025)\n    \n    In vivo MRI has revealed that common shrews shrink their brains for winter survival not through cell death, but by safely and reversibly losing water from their brain cells!\n  \n\n\n\n  \n    \n  \n  \n    \n      Gene expression reveals the pancreas of Aselli as a critical organ for plasma cell differentiation in common shrew (2025 preprint)\n    \n    First look at molecular pathways in the Aselli pancreas, suggesting a key role in immune cell differentiation during brain‚Äëplasticity cycles.\n  \n\n\n\n  \n    \n  \n  \n    \n      Seasonal and comparative evidence of adaptive gene expression in mammalian brain size plasticity (2025)\n    \n    Shrews shrink their brains to survive winter... and their hypothalamus helps pull it off! This study reveals seasonal gene shifts tied to energy balance, brain remodeling, and even some genes linked to human neurological disorders.\n  \n\n\n\n  \n    \n  \n  \n    \n      Captivity alters behaviour but not seasonal brain size change in semi-naturally housed shrews (2025)\n    \n    Captive shrews still shrink their brains seasonally... but their behavior tells a different story! Despite semi-natural captive conditions, they showed signs of chronic stress, with higher activity and lower motivation to learn, compared to wild ones. It shows how captivity can quietly reshape research outcomes.\n  \n\n\n\n  \n    \n  \n  \n    \n      Gene expression comparisons between captive and wild shrew brains reveal captivity effects (2025)\n    \n    Captivity alters how shrew brains work at the molecular level: hundreds of genes changed expression after just two months! These shifts echo patterns seen in human disorders like depression and neurodegeneration, raising caution for interpreting data from captive animals.\n  \n\n\n\n  \n    \n  \n  \n    \n      From summer growth to winter decline: brain size, captive effect & cognitive outcomes in common shrew during Dehnel‚Äôs phenomenon (2024)\n    \n    In my dissertation, I explore how seasonal brain plasticity affects learning and physiology in the common shrew!\n  \n\n\n\n  \n    \n  \n  \n    \n      Histological and MRI brain atlas of the common shrew, Sorex araneus, with brain region-specific gene expression profiles (2023)\n    \n    The common shrew is one of the few mammals that shrinks and regrows its brain... and now it has its own detailed brain atlas! This openly available resource combines histology, MRI, and gene expression to unlock the shrew brain.\n  \n\n\n\n  \n    \n  \n  \n    \n      Intra‚Äë and interspecific variation in self‚Äëcontrol capacities of parrots in a delay of gratification task (2022)\n    \n    This study compared self‚Äëcontrol across four parrot species using a delayed‚Äëgratification task. Findings highlight how feeding ecology and social structure may shape impulse control."
  },
  {
    "objectID": "content/publications.html#selected-publications",
    "href": "content/publications.html#selected-publications",
    "title": "Cecilia Baldoni",
    "section": "",
    "text": "Programmed seasonal brain shrinkage in the common shrew via water loss without cell death (2025)\n    \n    In vivo MRI has revealed that common shrews shrink their brains for winter survival not through cell death, but by safely and reversibly losing water from their brain cells!\n  \n\n\n\n  \n    \n  \n  \n    \n      Gene expression reveals the pancreas of Aselli as a critical organ for plasma cell differentiation in common shrew (2025 preprint)\n    \n    First look at molecular pathways in the Aselli pancreas, suggesting a key role in immune cell differentiation during brain‚Äëplasticity cycles.\n  \n\n\n\n  \n    \n  \n  \n    \n      Seasonal and comparative evidence of adaptive gene expression in mammalian brain size plasticity (2025)\n    \n    Shrews shrink their brains to survive winter... and their hypothalamus helps pull it off! This study reveals seasonal gene shifts tied to energy balance, brain remodeling, and even some genes linked to human neurological disorders.\n  \n\n\n\n  \n    \n  \n  \n    \n      Captivity alters behaviour but not seasonal brain size change in semi-naturally housed shrews (2025)\n    \n    Captive shrews still shrink their brains seasonally... but their behavior tells a different story! Despite semi-natural captive conditions, they showed signs of chronic stress, with higher activity and lower motivation to learn, compared to wild ones. It shows how captivity can quietly reshape research outcomes.\n  \n\n\n\n  \n    \n  \n  \n    \n      Gene expression comparisons between captive and wild shrew brains reveal captivity effects (2025)\n    \n    Captivity alters how shrew brains work at the molecular level: hundreds of genes changed expression after just two months! These shifts echo patterns seen in human disorders like depression and neurodegeneration, raising caution for interpreting data from captive animals.\n  \n\n\n\n  \n    \n  \n  \n    \n      From summer growth to winter decline: brain size, captive effect & cognitive outcomes in common shrew during Dehnel‚Äôs phenomenon (2024)\n    \n    In my dissertation, I explore how seasonal brain plasticity affects learning and physiology in the common shrew!\n  \n\n\n\n  \n    \n  \n  \n    \n      Histological and MRI brain atlas of the common shrew, Sorex araneus, with brain region-specific gene expression profiles (2023)\n    \n    The common shrew is one of the few mammals that shrinks and regrows its brain... and now it has its own detailed brain atlas! This openly available resource combines histology, MRI, and gene expression to unlock the shrew brain.\n  \n\n\n\n  \n    \n  \n  \n    \n      Intra‚Äë and interspecific variation in self‚Äëcontrol capacities of parrots in a delay of gratification task (2022)\n    \n    This study compared self‚Äëcontrol across four parrot species using a delayed‚Äëgratification task. Findings highlight how feeding ecology and social structure may shape impulse control."
  },
  {
    "objectID": "content/contact.html#get-in-touch",
    "href": "content/contact.html#get-in-touch",
    "title": "Cecilia Baldoni",
    "section": "Get in Touch",
    "text": "Get in Touch\nIf you‚Äôd like to reach out, feel free to contact me via email or through my social media platforms.\n\nEmail\nüñ•Ô∏èüìß cbaldoni@ab.mpg.de üôãÔ∏è üìß cecilia.baldons@gmail.com\n\n\nSocial Media\nüîó LinkedIn  üì∑ Instagram  üêô GitHub  ‚òïKo-fi\nI look forward to hearing from you!"
  },
  {
    "objectID": "content/open-science.html#open-science",
    "href": "content/open-science.html#open-science",
    "title": "Cecilia Baldoni",
    "section": "Open Science",
    "text": "Open Science\nOpen science is about building a scientific culture that values accessibility, transparency, reproducibility, and collaboration.\nThis is something I care about deeply and try to reflect in my daily work.  My contributions to open science span different levels, from personal practices to community involvement and infrastructure.   I see open science as a way of working, not just a set of rules. It affects how I design studies, write code, share results, and support others in doing the same.  \n\nWhat I share\nI make my data, analysis scripts, and visualizations publicly available whenever possible. Here is a list of resources where you can find my data and code: \n\nGitHub: my profile and open repositories, it includes some presentations for talks and workshops I organised.  ‚Üí See Workshop and Talks for more! \nOSF - Open Science Framework: my profile and materials from ongoing or not-yet-published projects. \nEdmond ‚Äì Max Planck Society Repository: in here you can find my data and scripts associated with peer-reviewed articles. \n\n\n\nCommunity & Initiatives\nOpen science also means supporting the systems and communities that make it possible. I contribute through organizing, reviewing, and community-building:\n\nI am an Open Science Ambassador in the Max Planck Society, helping build local awareness and promote transparent research practices across the Max Planck Institutes.\nI am an active member of SORTEE (Society for Open, Reliable, and Transparent Ecology and Evolution), where I serve on the Member Engagement Committee.\nIn 2025, I am organising the SORTEE Code Club, a monthly online meetup exploring practical tools and workflows for open science.\n‚Üí See the upcoming schedule here.\nI also work as a Data Editor for PCI Ecology, where I check the accessibility and reproducibility of data and code associated with recommended preprints."
  },
  {
    "objectID": "blog/notetaking-part1.html",
    "href": "blog/notetaking-part1.html",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "",
    "text": "It was January. I had finished my PhD the previous November and, after taking all of December off for a much-needed (and probably longer than it should‚Äôve been) holiday, I started the new year with a slow-dawning realisation.\nI guess it could be summarised in: ‚Äúand now, what?‚Äù.\nDespite a well documented history of growth (I was a doctor for crying out loud!), I couldn‚Äôt clearly answer two simple questions: What am I actually good at? And what do I enjoy doing?\nThe paralysis was real. I could list my accomplishments, but they felt like monolithic blocks. ‚ÄúI wrote a dissertation‚Äù could be a great conversation starter, but it‚Äôs not necessarily clear what that entails.\nTake, for example, ‚Äúwriting a scientific article‚Äù. It‚Äôs a huge part of academic life, but do I like it? Am I good at it?\nThe question itself is flawed because the task is too broad. ‚ÄúWriting an article‚Äù is a galaxy of smaller tasks: data collection, writing a draft, collaborating back-and-forth with co-authors (do they even read my innumerable emails at some point?), documenting everything because I forget what I am doing 3 hours later, making figures, endless reviews (spoiler alert: I hate that part). Is it possible to assign a single score for enjoyment or skill to ‚Äúwriting an article‚Äù?\nSo I decided to try a simple weekly review: every Friday, I‚Äôd sit down and try to reflect on my work week. Which tasks did I do? Did I like doing them? Was I skilled enough?\nThe reality? When I finally sat down with my thoughts I could barely remember a single task from the previous five days. I knew I had been busy, I knew I had worked, but when I tried to recall what I‚Äôd actually done, my brain just filled with static. It was as if the entire week had passed in a blur with no imprints on my memory.¬†\n\n\n\n\n\n\nA gif of Homer Simpson‚Äôs brain showing an old school cartoon going on inside his empty head\n\n\n\n\nFigure¬†1: Disclaimer: There‚Äôs a better clip of Abe Simpson with static in his brain, but I couldn‚Äôt find it, so Homer will have to do.\n\n\n\nThat‚Äôs when it hit me: I‚Äôve spent years taking notes on things in Logseq. If I can take notes on everything else, why not on my own work? That‚Äôs when I decided to turn my note-taking flow inward, and build a system for getting to know myself better."
  },
  {
    "objectID": "blog/notetaking-part1.html#the-ppp-post-phd-panic",
    "href": "blog/notetaking-part1.html#the-ppp-post-phd-panic",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "",
    "text": "It was January. I had finished my PhD the previous November and, after taking all of December off for a much-needed (and probably longer than it should‚Äôve been) holiday, I started the new year with a slow-dawning realisation.\nI guess it could be summarised in: ‚Äúand now, what?‚Äù.\nDespite a well documented history of growth (I was a doctor for crying out loud!), I couldn‚Äôt clearly answer two simple questions: What am I actually good at? And what do I enjoy doing?\nThe paralysis was real. I could list my accomplishments, but they felt like monolithic blocks. ‚ÄúI wrote a dissertation‚Äù could be a great conversation starter, but it‚Äôs not necessarily clear what that entails.\nTake, for example, ‚Äúwriting a scientific article‚Äù. It‚Äôs a huge part of academic life, but do I like it? Am I good at it?\nThe question itself is flawed because the task is too broad. ‚ÄúWriting an article‚Äù is a galaxy of smaller tasks: data collection, writing a draft, collaborating back-and-forth with co-authors (do they even read my innumerable emails at some point?), documenting everything because I forget what I am doing 3 hours later, making figures, endless reviews (spoiler alert: I hate that part). Is it possible to assign a single score for enjoyment or skill to ‚Äúwriting an article‚Äù?\nSo I decided to try a simple weekly review: every Friday, I‚Äôd sit down and try to reflect on my work week. Which tasks did I do? Did I like doing them? Was I skilled enough?\nThe reality? When I finally sat down with my thoughts I could barely remember a single task from the previous five days. I knew I had been busy, I knew I had worked, but when I tried to recall what I‚Äôd actually done, my brain just filled with static. It was as if the entire week had passed in a blur with no imprints on my memory.¬†\n\n\n\n\n\n\nA gif of Homer Simpson‚Äôs brain showing an old school cartoon going on inside his empty head\n\n\n\n\nFigure¬†1: Disclaimer: There‚Äôs a better clip of Abe Simpson with static in his brain, but I couldn‚Äôt find it, so Homer will have to do.\n\n\n\nThat‚Äôs when it hit me: I‚Äôve spent years taking notes on things in Logseq. If I can take notes on everything else, why not on my own work? That‚Äôs when I decided to turn my note-taking flow inward, and build a system for getting to know myself better."
  },
  {
    "objectID": "blog/notetaking-part1.html#we-need-clay-or-my-citations-are-always-messed-up",
    "href": "blog/notetaking-part1.html#we-need-clay-or-my-citations-are-always-messed-up",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "‚ÄúWe Need Clay!‚Äù (or: my citations are always messed up)",
    "text": "‚ÄúWe Need Clay!‚Äù (or: my citations are always messed up)\nYou know how Sherlock Holmes said, ‚ÄúData! Data! Data! I can‚Äôt make bricks without clay‚Äù?\nWell, I didn‚Äôt. I had to google it because my brain had somewhat summarized it in ‚ÄúI need clay!‚Äù, which didn‚Äôt make much sense when I wrote it down. I blame my internal Italian-to-English translator.\nEither way, as a good scientist, I knew I needed data.\nI decided to stick with Logseq for many reasons.\nFirst of all, I love Logseq.\nSecondly, I spent years building a routine of note-taking, so I could just add an additional step. Logseq was the hub for my PhD research, it is where I plan these blogpost, my work tasks and anything important in my life. But one thing is to write things down and another one is to read them again and do something with them (more on this topic coming soon, stay tuned for Part 2 on this note-taking blogpost series).\nWhen I started this ‚Äúproject‚Äù I had a very loose plan. The first important rule was to put all the entries together. I chose the tag #SelfReflection (mostly because #journaling was already taken for other things).\nIn the project page, I tried to summarize the plan it in few bullet points:\n\n\n\n\n\n\nPhases\n\n\n\n\nData collection: I need data to work with. Record the tasks or activities done throughout the week and write down how I feel about them, but also if I feel skilled at them or not.\n\nMandatory: Write down small and specific tasks. For example, instead of ‚Äúwrote article‚Äù say ‚Äúwrote 200-word paragraph about the function of AQP4‚Äù\n\nReview: After the week is over, check the entries in Logseq for recurrent patterns. For example, things that weren‚Äôt real tasks, but did them because I wanted to, needed to, or enjoyed it.\nCreate keywords from the data: Having specific tasks I enjoy/am good at, I can now extract skills.\n\nE.g., ‚ÄúI like explaining concepts visually‚Äù ‚Üí ‚ÄúVisual Communication, Science Communication‚Äù\nE.g., ‚ÄúI enjoy breaking down complex topics‚Äù ‚Üí ‚ÄúContent Structuring, Information Design‚Äù\nE.g., ‚ÄúI love to create systems for organizing data‚Äù ‚Üí ‚ÄúKnowledge Management‚Äù\n\nRinse and repeat\n\n\n\nAs you can see, my initial system was quite simple.\nAnd, as you might have noticed, I already left you with a spoiler: after a few weeks I edited the first point to remind myself that too broad tasks were not the point. But that‚Äôs the good thing about trying a system, you can always improve it!\nFor Version 1.0 I created a template that I could call anywhere in my notes:\n\n\n\n\n\n\nVersion 1.0\n\n\n\n\n#SelfReflection\n\nTask:\nFeeling:\nEnjoyment:\nSkill Level:\n\n\n\n\nTo make this a real habit, I embedded this empty structure directly into my daily notes template in Logseq. Every morning as I was opening Logseq, it was there.\nI won‚Äôt lie, the beginning was extremely difficult. I‚Äôm sure I‚Äôm not alone in this: you finish your workday, you‚Äôre mentally exhausted, but you cannot recollect what you actually did.\nBut it‚Äôs like meditation; everyone sucks at it at the beginning. With practice, the process became easier, and I got better at acknowledging the small tasks: ‚ÄúToday I wrote a difficult email, I felt super good about it, even if I wasn‚Äôt feeling very skilled.‚Äù Done. One data point.\nFor a while, this worked. Then, a new kind of excitement took hold. I wasn‚Äôt just journaling anymore; I was building a dataset.\nMindblowing sound.\nAnd if you have a dataset‚Ä¶ you have to plot it. I think it‚Äôs a law somewhere, look it up.\nThis decision to move my notes into R for analysis was the turning point that exposed every flaw in my simple little system."
  },
  {
    "objectID": "blog/notetaking-part1.html#the-roadblocks-in-r",
    "href": "blog/notetaking-part1.html#the-roadblocks-in-r",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "The Roadblocks in R",
    "text": "The Roadblocks in R\nAfter a few months, I had a reasonable amount of data and was ready to make the jump to R. To make this journey reproducible for you, I‚Äôll use a small, fake dataset that perfectly mirrors the structure of my real data (for the whole workflow, check the last section!):\n\nlibrary(tidyverse)\n\n# A reproducible example of the data (after extraction)\n# this mimics what the parsing script would produce\n\nfake_reflections &lt;- tibble::tribble(~task, ~feeling_score, ~enjoyment, ~skill,\n                                    \"Wrote a difficult email\", 5, 4, 6,\n                                    \"Gave Tidyverse workshop\", 7, 9, 10,\n                                    \"Debugged failing script\", 4, 8, 9,\n                                    \"Organized project files\", 6, 7, 8)\n\nThe first problem was immediately obvious. I should have seen this coming. My Task descriptions were unique and messy, with no repeatable patterns: great for a human reader but useless for aggregation.\nThis led to the first iteration: creating a Tag field for consistent taxonomy for my work.\nThis meant exactly what you are thinking: going back to all my entries and add meaningful tags.\nYuppi, she said sarcastically.\n\n\n\n\n\n\nVersion 2.0\n\n\n\n\n#SelfReflection\n\nTask:\nFeeling:\nEnjoyment:\nSkill Level:\nTag:\n\n\n\n\n\nlibrary(tidyverse)\n# Same fake data, but with tag as well\n\nfake_reflections &lt;- tibble::tribble(~task, ~feeling_score, ~enjoyment, ~skill, ~tag, \n                                    \"Wrote a difficult email\", 5, 4, 6, \"writing\", \n                                    \"Gave Tidyverse workshop\", 7, 9, 10, \"presentation; mentoring\",\n                                    \"Debugged failing script\", 4, 8, 9, \"coding\", \n                                    \"Organized project files\", 6, 7, 8, \"organisation; project management\")\n\nNice! Now I can extract the information and plot it!\n\nfake_reflections &lt;- fake_reflections %&gt;%\n  separate_rows(tag, sep = \"\\\\s*;\\\\s*\") %&gt;%\n  mutate(tag = str_to_title(tag))\n\nggplot(fake_reflections, aes(x = skill, y = enjoyment, color = tag)) +\n  geom_jitter(size=3)\n\n\n\n\n\n\n\n\nOh. Wait a minute.\n\nstr(fake_reflections)\n\ntibble [6 √ó 5] (S3: tbl_df/tbl/data.frame)\n $ task         : chr [1:6] \"Wrote a difficult email\" \"Gave Tidyverse workshop\" \"Gave Tidyverse workshop\" \"Debugged failing script\" ...\n $ feeling_score: num [1:6] 5 7 7 4 6 6\n $ enjoyment    : num [1:6] 4 9 9 8 7 7\n $ skill        : num [1:6] 6 10 10 9 8 8\n $ tag          : chr [1:6] \"Writing\" \"Presentation\" \"Mentoring\" \"Coding\" ...\n\n\nI had duplicate entries in my tibble! How?\nWell, better data wrangling people would know already, but it took me a minute to discover the ‚Äúproblem‚Äù (if we want to call it problem).\nI used multiple tags per task, thinking it would best capture my experience. If I am giving a workshop, it‚Äôs both ‚ÄúMentoring‚Äù and ‚ÄúPresentation‚Äù, why choose? It is different than, for example, giving a presentation to a lab meeting. Mentoring could also be other things, like helping a student prepare with their application or interview. I was pretty happy with my recordings.\nBut when you run separate_rows(tag, sep = \"\\s*;\\s*\"), for every row with multiple tags (e.g.¬†‚ÄúMentoring‚Äù and ‚ÄúPresentation‚Äù), it creates a new row per tag, duplicating all the other fields (task, feeling, etc.) for each tag. So, if a task has two tags, you get two rows (one for each tag, but all fields except tag are exactly duplicated). For three tags, three rows. For one tag, no duplication.\nAargh. What now? some entries, like the Gave Tidyverse workshop, were now duplicated, skewing my data. My clever system was an analytical headache. That real-world problem forced me back to the drawing board to weigh the trade-off between keeping this system (it‚Äôs not that bad) or clarity.\nI considered a few options:\n\nKeep the multiple tags and duplicated rows.\nForce myself to choose only one primary tag per task.\nIf a task truly had multiple facets, break it into smaller, single-tag tasks.\nAdd a Tag2 field for secondary tags.\n\nI can see the pro and cons for all options. I wondered about what to do. The reason why I wrote it all here it‚Äôs because, ultimately, the best option for me could not be the best option for someone else. This is the beauty of Personal Knowledge Management (PKM): ‚Äúwhat works for me might not work for you‚Äù.\nThe key is to find a system that serves your analytical goals. Do you mind the multiple rows? Maybe not. I can see a use of it: for example if you want to see which Tags are often paired together, and if they are often associated with high enjoyment score.\nBut I am not so interested in that, so I chose option 3. It forced me to be more specific about what I was actually doing. If I couldn‚Äôt choose a single tag, the task was probably too big anyway."
  },
  {
    "objectID": "blog/notetaking-part1.html#defining-what-matters",
    "href": "blog/notetaking-part1.html#defining-what-matters",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "Defining what matters",
    "text": "Defining what matters\nWith my tagging system refined, I was ready. But then I hit my nth roadblock, and I realized I wasn‚Äôt.\nNamely, I looked back at my data and realized I was using Feeling and Enjoyment interchangeably. That is not a good sign. There was a reason I created two fields, but what was that again? And why didn‚Äôt I write it down?\nI had to pause and create distinct definitions for myself:\n\n\n\n\n\n\nNote\n\n\n\nFeeling: What was my emotional state during or just after the task? This is about the immediate mood (e.g.¬†energized, frustrated, proud, anxious). It can be influenced by deadlines or context.\nEnjoyment: Did I actually like the task itself? e.g.¬†you can feel accomplished after a task you found profoundly boring.\n\n\nHere‚Äôs how the (my) distinction plays out:\n\nYou might feel stressed while troubleshooting code (Feeling: ‚Äúanxious, challenged‚Äù), but deeply enjoy the problem-solving (Enjoyment: 8/10).\nYou might feel calm while doing mundane admin work (Feeling: ‚Äúrelaxed‚Äù), but find it intensely boring (Enjoyment: 2/10).\n\nThis clarification fundamentally improved my data. I started noticing tasks where my Feeling was low but Enjoyment was high. For example, I often felt anxious before a presentation (Feeling score: 3/10), but I consistently rated my Enjoyment of the act of presenting as very high (9/10). This simple distinction revealed that some of the activities that are most rewarding for me are also the ones that push me out of my comfort zone. In this case, I maintained Feeling as a numerical score, but I introduced a categorical tag alongside it:\n\nlibrary(tidyverse)\n\nfake_reflections &lt;- tibble::tribble(~task, ~feeling_score, ~feeling_text, ~enjoyment, ~skill, ~tag,\n                                    \"Wrote a difficult email\",  5, \"focused\", 4, 6, \"writing\",\n                                    \"Gave Tidyverse workshop\",  3, \"anxious\", 9, 10, \"presentation\",\n                                    \"Debugged failing script\",  4, \"challenged\", 8, 9, \"coding\",\n                                    \"Organized project files\",  7, \"productive\", 7, 8, \"project management\")\n\nThis is still a working system. I am not sure yet what do I want to prioritize: the numerical score or the categorical tag, or perhaps find a way to integrate both. I need more data collection before making a decision. But ehi, that‚Äôs exactly what I love about taking notes! It‚Äôs like this system that‚Äôs always growing and getting better as I figure things out more about my own experiences."
  },
  {
    "objectID": "blog/notetaking-part1.html#a-system-for-self-knowledge-and-whats-next",
    "href": "blog/notetaking-part1.html#a-system-for-self-knowledge-and-whats-next",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "A system for self-knowledge (and what‚Äôs next)",
    "text": "A system for self-knowledge (and what‚Äôs next)\nThis whole process taught me a crucial lesson: building and refining the note-taking system was the self-reflection. The journey of wrestling with my messy data forced me to define what I do, how I categorize my work. And in the end, I finally got my plot!\n\nlibrary(tidyverse)\n\n# A more thorough fake dataset\n\nreflections_final &lt;- tibble::tribble(~task, ~enjoyment, ~skill, ~tag,\n  \"Gave Tidyverse workshop\", 9, 10, \"Presentation\",\n  \"Debugged a complex script\", 8, 9, \"Coding\",\n  \"Mentored a junior colleague on their project\", 9, 8, \"Mentoring\",\n  \"Outlined a new blog post idea\", 8, 7, \"Writing\",\n  \"Designed a data visualization\", 9, 9, \"Coding\",\n  \"Automated a boring report with a script\", 10, 8, \"Coding\",\n  \"Addressed reviewer comments (round 3)\", 2, 9, \"Writing\",\n  \"Formatted citations for a paper\", 3, 10, \"Admin\",\n  \"Answered 50 emails in one afternoon\", 4, 8, \"Admin\",\n  \"Chased co-authors for manuscript feedback\", 2, 7, \"Project Management\",\n  \"Wrote detailed project documentation\", 5, 8, \"Writing\",\n  \"Learned a new ggplot extension\", 8, 4, \"Learning\",\n  \"First attempt at a simple Shiny app\", 9, 5, \"Coding\",\n  \"Networking at a conference\", 7, 5, \"Networking\",\n  \"Trying out a new note-taking method\", 8, 3, \"PKM\",\n  \"Recording a short tutorial video\", 7, 4, \"Presentation\",\n  \"Manually copied data from PDF to Excel\", 1, 3, \"Data Entry\",\n  \"Sat through a mandatory, irrelevant meeting\", 2, 2, \"Meetings\",\n  \"Troubleshot a printer connection issue\", 1, 1, \"Admin\",\n  \"Filled out travel reimbursement forms\", 3, 4, \"Admin\",\n  \"Unsuccessfully tried to configure a server\", 2, 3, \"Coding\")\n\n# The ggplot code remains the same\n\nggplot(reflections_final, aes(x = skill, y = enjoyment, color = tag)) +\n  geom_jitter(size = 4) +\n  labs(x = \"Skill Level\", y = \"Enjoyment\",\n    color = \"Task Type\") +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, 2)) +\n  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 2)) +\n  geom_hline(yintercept = 5, linetype = \"dashed\", color = \"gray50\") +\n  geom_vline(xintercept = 5, linetype = \"dashed\", color = \"gray50\") +\n    annotate(\"text\", x = 7.5, y = 7.5, label = \"Zone of Genius\", color = \"gray20\", fontface = \"bold\") +\n  annotate(\"text\", x = 7.5, y = 2.5, label = \"Burnout Zone\", color = \"gray20\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.5, y = 7.5, label = \"Growth Zone\", color = \"gray20\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.5, y = 2.5, label = \"Toil Zone\", color = \"gray20\", fontface = \"bold\")\n\n\n\n\n\n\n\n\nThis simple scatter plot is a map of my professional self.\n\nMy Zone of Genius (High Skill, High Enjoyment): For me, this is where Coding, Presentation, and Mentoring live.\nThe Burnout Zone (High Skill, Low Enjoyment): Things I‚Äôm good at but drain me.\nThe Growth Zone (Low Skill, High Enjoyment): Things I love and should invest more time in.\nThe Toil Zone (Low Skill, Low Enjoyment): Tasks to minimize/delegate/automate.\n\nThat‚Äôs it. This journey gave me a powerful quantitative map of my skills and passions.\nBut what about the stories hidden in the unstructured text of my daily notes? What themes and patterns emerge when I‚Äôm not scoring and tagging, but just writing?\nThat‚Äôs a story for another time. In Part 2, we‚Äôll leave the numbers behind and venture into the world of text mining to see what other secrets my Logseq vault is waiting to tell me."
  },
  {
    "objectID": "blog/notetaking-part1.html#tldr",
    "href": "blog/notetaking-part1.html#tldr",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "TL;DR",
    "text": "TL;DR\n\nMotivation: To overcome post-PhD uncertainty, I needed to understand what work tasks I was good at and what I genuinely enjoyed.\nMethod: I used a daily template in Logseq to systematically record my work tasks, rating each on a 1-10 scale for Skill Level and Enjoyment, and adding descriptive Tags.\nIteration: Moving this data to R for visualization revealed flaws in my system. I had to refine my process by breaking down large tasks, solving a data-duplication issue caused by multiple tags, and creating clear definitions for my metrics (like Feeling vs.¬†Enjoyment).\nPayoff: The result is a scatter plot that visually maps my professional self into four key zones (e.g., ‚ÄúZone of Genius‚Äù), providing clear data on my career preferences.\nCore Lesson: The act of building and refining a personal tracking system is, in itself, a powerful form of self-reflection. The messy process is where the real learning happens."
  },
  {
    "objectID": "blog/notetaking-part1.html#for-the-adventurous-the-full-r-parsing-script",
    "href": "blog/notetaking-part1.html#for-the-adventurous-the-full-r-parsing-script",
    "title": "Note-Taking is more powerful than you think (1/2)",
    "section": "For the adventurous: the full R Parsing script",
    "text": "For the adventurous: the full R Parsing script\nHere is the function I use to extract the structured data from my Logseq journal files. You can adapt it for your own vault/folder.\n\n# Helper to safely extract a regex group\nextract_field &lt;- function(lines, pattern) {\n  match &lt;- str_match(lines, pattern)\n  if (!is.null(match) && nrow(match) &gt; 0) match[, 2] else NA_character_\n}\n\nextract_reflections &lt;- function(file) {\n  lines &lt;- readLines(file, warn = FALSE)\n  entries &lt;- list()\n  inside_block &lt;- FALSE\n  current_block &lt;- c()\n  \n  for (line in lines) {\n    if (grepl(\"^- +#SelfReflection\", line)) {\n      if (length(current_block) &gt; 0) entries &lt;- append(entries, list(current_block))\n      current_block &lt;- c(line)\n      inside_block &lt;- TRUE\n    } else if (inside_block) {\n      if (grepl(\"^\\\\s+\", line)) {\n        current_block &lt;- c(current_block, line)\n      } else if (line == \"\" || grepl(\"^- +[^#]\", line)) {\n        entries &lt;- append(entries, list(current_block))\n        current_block &lt;- c()\n        inside_block &lt;- FALSE\n      }\n    }\n  }\n  if (length(current_block) &gt; 0) entries &lt;- append(entries, list(current_block))\n  \n  map_dfr(entries, function(entry) {\n    task_line &lt;- entry[grepl(\"\\\\*\\\\*Task\\\\*\\\\*:\", entry)]\n    feeling_line &lt;- entry[grepl(\"\\\\*\\\\*Feeling\\\\*\\\\*:\", entry)]\n    enjoyment_line &lt;- entry[grepl(\"\\\\*\\\\*Enjoyment\\\\*\\\\*:\", entry)]\n    skill_line &lt;- entry[grepl(\"\\\\*\\\\*Skill Level\\\\*\\\\*:\", entry)]\n    tag_line &lt;- entry[grepl(\"\\\\*\\\\*Tag\\\\*\\\\*:\", entry)]\n    \n    task &lt;- extract_field(task_line, \"\\\\*\\\\*Task\\\\*\\\\*: *(.*)\")\n    feeling_score &lt;- as.numeric(extract_field(feeling_line, \"\\\\*\\\\*Feeling\\\\*\\\\*:\\\\s*(\\\\d+)\"))\n    feeling_text &lt;- extract_field(feeling_line, \"\\\\*\\\\*Feeling\\\\*\\\\*:\\\\s*\\\\d+\\\\s*[-‚Äì‚Äî]\\\\s*(.*)\")\n    enjoyment &lt;- as.numeric(extract_field(enjoyment_line, \"\\\\*\\\\*Enjoyment\\\\*\\\\*: *(\\\\d+)\"))\n    skill &lt;- as.numeric(extract_field(skill_line, \"\\\\*\\\\*Skill Level\\\\*\\\\*: *(\\\\d+)\"))\n    tag &lt;- extract_field(tag_line, \"\\\\*\\\\*Tag\\\\*\\\\*:\\\\s*(.*)\")\n    \n    tibble(\n      file = basename(file),\n      task = task,\n      feeling_score = feeling_score,\n      feeling_text = feeling_text,\n      enjoyment = enjoyment,\n      skill = skill,\n      tag = tag\n    )\n  })\n}\n\n\n# To run it yourself:\n# logseq_path &lt;- \"/path/to/your/Logseq/journals\"\n# md_files &lt;- list.files(logseq_path, pattern = \"\\\\.md$\", full.names = TRUE)\n# reflections &lt;- map_dfr(md_files, extract_reflections)"
  },
  {
    "objectID": "blog/notetaking-part2.html",
    "href": "blog/notetaking-part2.html",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "In the first part of this series, I made a plot.\nWell ok. It was a bit more than that. It was a nice exploration of my work tasks based on self-recorded notes, and in the end‚Ä¶ yes, I got a nice, clean plot that put my skills and enjoyment into neat little boxes.\nI could have stopped there, satisfied with my color-coded chart. But here‚Äôs the thing about note-taking: it‚Äôs addictive. And I realized the real story wasn‚Äôt in the tidy, structured scores from Part 1, but was hidden in the unstructured chaos of my daily journal‚Ä¶ the digital equivalent of a junk drawer full of brilliant ideas, and existential questions about lunch.\nCould I find meaning in that mess? Let‚Äôs find out.\n\n\n\nThe gif i was looking for in the previous part, with Grampa Simpson thinking back and finding only static noise\n\n\nSmall notes before we begin:\n1. This post is structured in chapters, and it‚Äôs a deliberate choice. Since this whole adventure is about analyzing long-form writing, it felt right to structure the post itself like a book. We‚Äôre on a literary adventure here, people!\n2. Given the sensitive information in my private journal, I decided to paste the actual R code I used, but show you the output as a static image (.png). This was a conscious choice. I tried to create some ‚Äúfake entries‚Äù but I felt they did not really tell the story I wanted to share.\n\n\nI have to admit, I knew nothing about how to analyse long text entries. I had no idea what the process was called, or if it was possible to do it in R. I just knew I wanted to learn more about my writing: know which words I use more often, if there were interesting trends over time, or if I was getting happier (or sadder). Since I didn‚Äôt know how to put all of this into actual words, I did what I usually do in these situations: I described my goal to a Large Language Model.\nI should be happy they are not sentient yet, because it might have thought I was being obtuse and ridiculous.\nAfter writing down my question, it just replied: ‚ÄúDid you mean text mining?‚Äù\nEhm, I guess. Thank you.\nThe next obvious step was to find a guide. As a sucker for a good coding book, I was thrilled to find ‚ÄúText Mining with R‚Äù online, which became my manual for this adventure.\n\n\n\nIMPORTANT COMMUNICATION\nHere, I need to take a pause and acknowledge something important. Despite my love for Logseq, not everybody uses it (say whaaaat!). I debated with myself whether to add this section at all, and jump into the text mining part with a nice fake dataset created in R for the occasion. But reality is never that easy and tidy. So‚Ä¶ If you are following along, and you too take your long notes in Logseq (or Obsidian, for that matter), you might get to similar data wrangling impasses. For that reason, I decided to keep this part. If you have never used Logseq and you are just in for the ride, this might be less interesting for you, and you might want to skip to the part where the data is tidy and ready for analysis.\n\nBefore any analysis, I had to get my data out of Logseq and into R. This involves looping through all your journal files and pulling out any lines, and blocks, that contain your target tags (mine were #journaling, and #moodlog). This is very similar to the loop I wrote for the Part 1 of this series, I just swapped the tags:\n\nlibrary(tidyverse)\n\nlogseq_path &lt;- \"~/Logseq/journals\"\nmd_files &lt;- list.files(logseq_path, pattern = \"\\\\.md$\", full.names = TRUE)\n\nextract_entries &lt;- function(file_path) {\n  lines &lt;- readLines(file_path, warn = FALSE)\n  file_date &lt;- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %&gt;% ymd()\n  \n  result &lt;- list()\n  i &lt;- 1\n  \n  while (i &lt;= length(lines)) {\n    line &lt;- lines[i]\n    if (str_detect(line, \"moodlog|journaling\")) {\n      \n      matched_tag &lt;- str_extract(line, \"moodlog|journaling\")\n      entry_lines &lt;- c(line)\n      i &lt;- i + 1\n      \n      while (\n        i &lt;= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines &lt;- c(entry_lines, lines[i])\n        i &lt;- i + 1\n      }\n      \n      result &lt;- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  if (length(result) &gt; 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\n\nentries &lt;- map_df(md_files, extract_entries)\n\nAfter running this, you‚Äôre left with a data frame. But it‚Äôs not pretty: it‚Äôs a raw data dump full of markdown syntax with weird tabs (\\t), newlines (\\n), and automated metadata.\nHere is an example of how it might look like, built with some fake entries:\n\nThere are several problems when you import this from Logseq into R. If you like Logseq for its block structure, know that that becomes your nightmare. If you add new lines, or tags, or any other plugin feature, they are all gonna be written there.\nHere is what I did to clean up this mess.\nWeapon 1: fixed() for str_remove_all()\nOf course, the process wasn‚Äôt smooth. I hit a syntax error trying to clean a {renderer :wordcount_} tag from Logseq (side note: that‚Äôs the function I use to count the words in my long writing within Logseq. It‚Äôs pretty neat, unless you want to load the data into R). The problem is that the str_remove_all() function thinks I‚Äôm being difficult by giving it a complex search pattern. Apparently, the curly braces are confusing the function.\nThe fix? fixed() (drum roll sound), which tells R to stop trying to be clever and just find the literal text. I don‚Äôt know about you, but adding ‚Äúfixed‚Äù into a line of code makes it all better. It should be the default.\nWeapon 2: Surgical strikes on metadata\nThen there was the problem about the metadata. First, there was collapsed::true, which is just my way of telling Logseq to hide children blocks. It means if I step away from my laptop to get a coffee, my wandering thoughts don‚Äôt become public reading material for anyone walking by. The other one was :LOGBOOK: ... :END:, metadata automatically generated by Logseq‚Äôs time-tracking feature. It‚Äôs meant to record how long it passed from ‚Äúassigning a task‚Äù to completing it. They are structured metadata, so they will definitely contaminate the text analysis.\nWhat to do with those? Remove the entire lines. This is better than excluding the whole entry because it preserves any useful text that might be on the same line, while getting rid of all the noisy metadata.\nWeapon 3: str_squish()\nHave you heard about this function? It removes all the weird extra spaces from your text. It‚Äôs an absolute lifesaver.\n\nlibrary(tidyverse)\n\nentries &lt;- entries %&gt;%\n  mutate(\n    entry_text = entry_text %&gt;%\n      str_remove(\"journaling|moodlog\") %&gt;%\n      str_remove_all(\"\\\\n|\\\\t\") %&gt;%\n      str_remove_all(\"-|#|\\\\|\") %&gt;%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %&gt;%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %&gt;%\n      str_remove_all(\"collapsed:: true\") %&gt;%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %&gt;%\n      \n      str_squish()\n  )\n\n\n\n\n\nI had all the entries in R, I had a date column, and a text column. What now?\nWell, let‚Äôs follow the book.\nThe tidy text book uses as an example Jane Austen books, from the package janeaustenr. Of course, using great literature would be way cooler (more words, for one, and probably better words too), but we will battle imposter syndrome and convince ourselves that our entries have nothing to compete with.\nWell, no offense to Jane Austen, but my own messy journal entries are the most interesting part of this project for me.\nThe first important step is to remove stop_words, meaning all the articles, prepositions and ‚Äúfillers‚Äù words. You can use a specific dataset embedded in tidytext. Without the filler words, what remains it‚Äôs only important stuff (well, let‚Äôs see!).\nWhich is the most used word in my journal entries?\n\nlibrary(tidytext)\n\nwords_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  anti_join(stop_words) %&gt;% \n  count(word, sort = TRUE)\n\nwords_counted  %&gt;% \n  filter(n &gt; 6) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nFun fact! From my own entries the most used word was ‚ÄúTime‚Äù. Which I found very intriguing.\nA funnier fact? That‚Äôs the second most used word in the Jane Austen books! Take that Jane Austen!\nActually, it‚Äôs also the first word for Bront√´ and Horwell, isn‚Äôt that interesting?\nWant to do this yourself? The core of it is using unnest_tokens() to create one word per row and count() to tally them up.\n\n\n\nKnowing what I write about is interesting, but the next logical step is to figure out how I felt when I was writing it. This is where sentiment analysis comes in. There are several different methods, or ‚Äúlexicons,‚Äù available in R that assign positive or negative scores to words. I tried many using my dataset, but for simplicity, here I will use the one called ‚ÄúAFINN‚Äù.\nThe AFINN lexicon is essentially a word list where each word is rated on a scale from -5 (very negative) to +5 (very positive). The process is quite simple: the code goes through each words in my journal entries, finds all the words that exist in the AFINN list, and then adds up their scores. An entry with words like ‚Äúamazing‚Äù and ‚Äúhappy‚Äù would get a high positive score, while an entry with ‚Äúterrible‚Äù and ‚Äústress‚Äù would get a negative one.\nThe get_sentiment() function from the syuzhet package does all this work for you, creating a single sentiment score for each day‚Äôs entry.\nThe first thing I did was plot the sentiment score of my entries over time. This is where the story took another sharp turn.\n\nlibrary(syuzhet)\n\nsentiment &lt;- entries %&gt;%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Plot sentiment over time\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n\n\nIs it my impression or is there a bit of a gap? How come?\nThen I looked at the dates and realised: that‚Äôs when I was writing my dissertation! I remember now, the tag I was using during that time was #Dissertation Journal, to distinguish it from other entries. I should add those entries too. Well, easy enough, I could re-run my loop, adding an additional tag!\n\nextract_entries &lt;- function(file_path) {\n  lines &lt;- readLines(file_path, warn = FALSE)\n  file_date &lt;- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %&gt;% ymd()\n  \n  result &lt;- list()\n  i &lt;- 1\n  \n  while (i &lt;= length(lines)) {\n    line &lt;- lines[i]\n    if (str_detect(line, \"moodlog|journaling|Dissertation Journal\")) { #Add the additional tag!\n      \n      matched_tag &lt;- str_extract(line, \"moodlog|journaling|Dissertation Journal\") #Here too!\n      entry_lines &lt;- c(line)\n      i &lt;- i + 1\n      \n      while (\n        i &lt;= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines &lt;- c(entry_lines, lines[i])\n        i &lt;- i + 1\n      }\n      \n      # Save entry\n      result &lt;- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  if (length(result) &gt; 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\nentries &lt;- map_df(md_files, extract_entries)\n\nentries &lt;- entries %&gt;%\n  mutate(\n    entry_text = entry_text %&gt;%\n      str_remove(\"journaling|moodlog|Dissertation Journal\") %&gt;%\n      str_remove_all(\"\\\\n|\\\\t\") %&gt;%\n      str_remove_all(\"-|#|\\\\|\") %&gt;%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %&gt;%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %&gt;%\n      str_remove_all(\"logseq.orderlisttype::\") %&gt;% \n      \n      str_remove_all(\"collapsed:: true\") %&gt;%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %&gt;% \n      \n      str_squish()\n  )\n\nsentiment &lt;- entries %&gt;%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Re-plot\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n\n\nThis seems about right! I am happy I left the tags in the dataset now, I could see the evolution of my journaling, from moodlog to journaling, with a big sidetrack project in the middle!\nThe tag column could also have a new meaning. I could now FOR EXAMPLE visualise if I was more prone to use negative words while writing my dissertation (no, we are not going there. It was just a thought).\nBut with these new entries, what about my most used words now?\n\n# Re-run word frequency with the new tag\n\nwords_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  anti_join(stop_words) %&gt;% \n  count(word, sort = TRUE)\n\nwords_counted  %&gt;% \n  filter(n &gt; 20) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nAfter including my dissertation entries, the word frequency analysis had completely changed. My most used word was not time anymore, but paper.\nLOL. Yes, I literally laughed out loud on this one.\n\n\nTo understand what was driving these scores, I wanted to see the most common positive and negative words. The tidytext book has a fantastic visualization for this: a comparison word cloud.\n\npositive_negative_words &lt;- entries %&gt;%\n  # i need to tokenize into single words\n  unnest_tokens(word, entry_text) %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;%\n  count(word, sentiment, sort = TRUE)\n\n# We need to pivot the data into a matrix format:\n# one row per word, one column for \"positive\", one for \"negative\".\n\npositive_negative_words %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  comparison.cloud(\n    colors = c(\"firebrick\", \"steelblue\"),\n    max.words = 100\n  )\n\n#LOL shrew is a NEGATIVE word XD \n\n\nThis is where things get both interesting and messy.\nApparently ‚Äúshrew‚Äù is a negative word! I might need to thank Shakespeare for that. But when the shrew is your actual study species, the word doesn‚Äôt quite have the same meaning as in The Taming of the Shrew, right?\nBut I also see other problems. For example, ‚Äúexcel‚Äù could mean the verb, or it could mean I was frustrated with Microsoft Excel. ‚ÄúChronic‚Äù and ‚Äústress‚Äù likely refer to a study I was working on about chronic stress, not my chronic stress.\nAnd what about ‚Äúregression‚Äù? I am sure I never meant it negatively, I was probably talking about statistics (wait, that does not exclude the negative connotation tho!)\nIt‚Äôs fascinating. Have you ever wondered how many scientific terms have a negative meaning? ‚ÄúIssue‚Äù, ‚Äúregression‚Äù, ‚Äúplot‚Äù‚Ä¶ it‚Äôs a long list!\nI‚Äôm also amused by how often the word ‚Äúlike‚Äù appears on the positive side. Am I using it too much in my writing?\n\n\n\n\nAfter wrestling with the beautiful mess of the word cloud, I could finally stop looking at individual words and start searching for broader patterns in my own emotional week.\nWas there a rhythm to my moods? A secret logic to my good and bad days?\nThis is where Logseq‚Äôs structure really shines. Since every journal entry is automatically tied to a daily note, getting the day of the week is incredibly simple with the wday() function from the lubridate package. I decided to look at the average sentiment for each day of the week, and that‚Äôs when I found it. A clear, undeniable pattern.\nApparently, my week has a villain, and its name is Friday.\n\nlibrary(lubridate) # For wday()\n\n# It should have columns for 'date' and 'sentiment_score'\n\nworkweek_sentiment &lt;- sentiment %&gt;%\n  mutate(day_of_week = wday(date, label = TRUE, week_start = 1)) %&gt;%\n  group_by(day_of_week) %&gt;%\n  filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %&gt;% #let's remove saturday and sunday!\n  summarise(\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(workweek_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n\nweek_sentiment &lt;- sentiment %&gt;%\n  mutate(\n    # Get the day of the week from the date\n    day_of_week = wday(date, label = TRUE, week_start = 1)\n  ) %&gt;%\n  group_by(day_of_week) %&gt;%\n  #filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %&gt;% #let's remove saturday and sunday!\n  summarise(\n    # Calculate the average sentiment for each day\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(week_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  #geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n\n\n\nSide note: If you are wondering about the weeknames, just know they are in German!\nMy mood consistently takes a nosedive at the end of the work week, hitting a low on Friday. But the story gets more interesting when you look at the weekend. The sentiment shoots up dramatically on Sundays. The contrast was so stark that I ended up making two plots: one showing just the weekdays, and another including the weekend for comparison.\nIt was one of those moments where data confirms a vague feeling you‚Äôve had for years.\nWho knew? Well, a part of me did, I guess. But seeing it as a data point makes it real. It‚Äôs no longer just ‚Äúa bad day‚Äù; it‚Äôs a predictable rhythm. And once you see the pattern, you can start to do something about it. This is a discovery I can now be mindful of: I could, for example, schedule lighter tasks for Fridays or just be a little kinder to myself when the end-of-week slump hits.\n\n\nAfter looking at my main themes, I got curious. What about the stop_words? Could they tell a story too?\nI started thinking about it when I saw the word ‚Äúlike‚Äù so often repeated. I am not sure I use it in my writing as a verb, but more like‚Ä¶ well, exactly like that. I would find it interesting to investigate which stop words I use more often, and if I should rethink the way I write.\n\n# Find and count only the stop words\nstop_words_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  # Keep only the words that are in the stop_words list\n  inner_join(stop_words, by = c(\"word\" = \"word\")) %&gt;%\n  count(word, sort = TRUE)\n\nstop_words_counted %&gt;% \n  filter(n &gt; 50) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nOk, using the whole stop-words dataset might not have much information. The fact that ‚ÄúI‚Äù and ‚Äúthe‚Äù are the most used ones doesn‚Äôt tell me much on its own.\nBut I could filter out of this dataset. For example, I started with modal verbs. I wanted to see if I was a person of ‚Äòshoulds,‚Äô ‚Äòcoulds,‚Äô or ‚Äòwants.‚Äô Then I went over the pronouns, then conjunctions and time transition words.\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"should\", \"could\", \"would\", \"will\", \"want\", \"shall\", \"can\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"i\", \"me\", \"my\", \"myself\", \"you\", \"your\", \"he\", \"she\", \"they\", \"we\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"but\", \"however\", \"although\", \"because\", \"so\", \"therefore\", \"then\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"before\", \"after\", \"during\", \"since\", \"until\", \"ago\", \"within\"))  %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n   \n\n\n\n\nThe ‚ÄúText Mining with R‚Äù book is full of other amazing techniques, and I tried a couple more.\nI first tried word correlation to see which words I tend to associate with each other. The results were interesting but didn‚Äôt tell me anything I didn‚Äôt already expect (yes, dissertation is correlated with paper or writing).\nNext, I tried topic modeling to see if an algorithm could automatically find the hidden themes in my journal. This didn‚Äôt work as well as I‚Äôd hoped, likely because my journal entries are often short and context-heavy.\nChecking numbers with bigram was another interesting topic: I had a lot of numbers in my entries (often coupled with ‚Äúminutes‚Äù or ‚Äúweeks‚Äù, or maybe plain schedules) and I tried to couple numbers with their words, but that didn‚Äôt work: I somehow filtered some information from my dataset (the shrews ID from my dissertation have are unique number combinations) so that was a bit of a bummer.\nWhile these didn‚Äôt produce mind-blowing results for my specific dataset, they are incredibly powerful techniques. If you want to learn more, I highly recommend checking out the chapters on word correlation and topic modeling in the ‚ÄúTidy Text Mining with R‚Äù book!\n\n\nI approached this project as a beginner, wanting to see if I could find a story in the massive amount of text in my Logseq vault. The mission was biased, of course; I wanted to learn something about myself.\nAnd I did. I learned that my focus shifted dramatically during my dissertation, that my mood has a weekly rhythm, and that the tools to find these insights are accessible to anyone willing to learn.\nWe all have a massive, unstructured dataset of our own lives in our notes, emails, or calendars. It might look like junk, but it‚Äôs also a goldmine. The tools to explore it are more accessible than ever, and the only prerequisite is a little bit of curiosity.\nSo, is note-taking that powerful? It‚Äôs powerful enough to turn forgotten thoughts into a map of who you are. And for me, that‚Äôs powerful enough."
  },
  {
    "objectID": "blog/notetaking-part2.html#chapter-1-defining-a-battle-strategy",
    "href": "blog/notetaking-part2.html#chapter-1-defining-a-battle-strategy",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "I have to admit, I knew nothing about how to analyse long text entries. I had no idea what the process was called, or if it was possible to do it in R. I just knew I wanted to learn more about my writing: know which words I use more often, if there were interesting trends over time, or if I was getting happier (or sadder). Since I didn‚Äôt know how to put all of this into actual words, I did what I usually do in these situations: I described my goal to a Large Language Model.\nI should be happy they are not sentient yet, because it might have thought I was being obtuse and ridiculous.\nAfter writing down my question, it just replied: ‚ÄúDid you mean text mining?‚Äù\nEhm, I guess. Thank you.\nThe next obvious step was to find a guide. As a sucker for a good coding book, I was thrilled to find ‚ÄúText Mining with R‚Äù online, which became my manual for this adventure.\n\n\n\nIMPORTANT COMMUNICATION\nHere, I need to take a pause and acknowledge something important. Despite my love for Logseq, not everybody uses it (say whaaaat!). I debated with myself whether to add this section at all, and jump into the text mining part with a nice fake dataset created in R for the occasion. But reality is never that easy and tidy. So‚Ä¶ If you are following along, and you too take your long notes in Logseq (or Obsidian, for that matter), you might get to similar data wrangling impasses. For that reason, I decided to keep this part. If you have never used Logseq and you are just in for the ride, this might be less interesting for you, and you might want to skip to the part where the data is tidy and ready for analysis.\n\nBefore any analysis, I had to get my data out of Logseq and into R. This involves looping through all your journal files and pulling out any lines, and blocks, that contain your target tags (mine were #journaling, and #moodlog). This is very similar to the loop I wrote for the Part 1 of this series, I just swapped the tags:\n\nlibrary(tidyverse)\n\nlogseq_path &lt;- \"~/Logseq/journals\"\nmd_files &lt;- list.files(logseq_path, pattern = \"\\\\.md$\", full.names = TRUE)\n\nextract_entries &lt;- function(file_path) {\n  lines &lt;- readLines(file_path, warn = FALSE)\n  file_date &lt;- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %&gt;% ymd()\n  \n  result &lt;- list()\n  i &lt;- 1\n  \n  while (i &lt;= length(lines)) {\n    line &lt;- lines[i]\n    if (str_detect(line, \"moodlog|journaling\")) {\n      \n      matched_tag &lt;- str_extract(line, \"moodlog|journaling\")\n      entry_lines &lt;- c(line)\n      i &lt;- i + 1\n      \n      while (\n        i &lt;= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines &lt;- c(entry_lines, lines[i])\n        i &lt;- i + 1\n      }\n      \n      result &lt;- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  if (length(result) &gt; 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\n\nentries &lt;- map_df(md_files, extract_entries)\n\nAfter running this, you‚Äôre left with a data frame. But it‚Äôs not pretty: it‚Äôs a raw data dump full of markdown syntax with weird tabs (\\t), newlines (\\n), and automated metadata.\nHere is an example of how it might look like, built with some fake entries:\n\nThere are several problems when you import this from Logseq into R. If you like Logseq for its block structure, know that that becomes your nightmare. If you add new lines, or tags, or any other plugin feature, they are all gonna be written there.\nHere is what I did to clean up this mess.\nWeapon 1: fixed() for str_remove_all()\nOf course, the process wasn‚Äôt smooth. I hit a syntax error trying to clean a {renderer :wordcount_} tag from Logseq (side note: that‚Äôs the function I use to count the words in my long writing within Logseq. It‚Äôs pretty neat, unless you want to load the data into R). The problem is that the str_remove_all() function thinks I‚Äôm being difficult by giving it a complex search pattern. Apparently, the curly braces are confusing the function.\nThe fix? fixed() (drum roll sound), which tells R to stop trying to be clever and just find the literal text. I don‚Äôt know about you, but adding ‚Äúfixed‚Äù into a line of code makes it all better. It should be the default.\nWeapon 2: Surgical strikes on metadata\nThen there was the problem about the metadata. First, there was collapsed::true, which is just my way of telling Logseq to hide children blocks. It means if I step away from my laptop to get a coffee, my wandering thoughts don‚Äôt become public reading material for anyone walking by. The other one was :LOGBOOK: ... :END:, metadata automatically generated by Logseq‚Äôs time-tracking feature. It‚Äôs meant to record how long it passed from ‚Äúassigning a task‚Äù to completing it. They are structured metadata, so they will definitely contaminate the text analysis.\nWhat to do with those? Remove the entire lines. This is better than excluding the whole entry because it preserves any useful text that might be on the same line, while getting rid of all the noisy metadata.\nWeapon 3: str_squish()\nHave you heard about this function? It removes all the weird extra spaces from your text. It‚Äôs an absolute lifesaver.\n\nlibrary(tidyverse)\n\nentries &lt;- entries %&gt;%\n  mutate(\n    entry_text = entry_text %&gt;%\n      str_remove(\"journaling|moodlog\") %&gt;%\n      str_remove_all(\"\\\\n|\\\\t\") %&gt;%\n      str_remove_all(\"-|#|\\\\|\") %&gt;%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %&gt;%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %&gt;%\n      str_remove_all(\"collapsed:: true\") %&gt;%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %&gt;%\n      \n      str_squish()\n  )"
  },
  {
    "objectID": "blog/notetaking-part2.html#chapter-2-what-am-i-actually-writing-about",
    "href": "blog/notetaking-part2.html#chapter-2-what-am-i-actually-writing-about",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "I had all the entries in R, I had a date column, and a text column. What now?\nWell, let‚Äôs follow the book.\nThe tidy text book uses as an example Jane Austen books, from the package janeaustenr. Of course, using great literature would be way cooler (more words, for one, and probably better words too), but we will battle imposter syndrome and convince ourselves that our entries have nothing to compete with.\nWell, no offense to Jane Austen, but my own messy journal entries are the most interesting part of this project for me.\nThe first important step is to remove stop_words, meaning all the articles, prepositions and ‚Äúfillers‚Äù words. You can use a specific dataset embedded in tidytext. Without the filler words, what remains it‚Äôs only important stuff (well, let‚Äôs see!).\nWhich is the most used word in my journal entries?\n\nlibrary(tidytext)\n\nwords_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  anti_join(stop_words) %&gt;% \n  count(word, sort = TRUE)\n\nwords_counted  %&gt;% \n  filter(n &gt; 6) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nFun fact! From my own entries the most used word was ‚ÄúTime‚Äù. Which I found very intriguing.\nA funnier fact? That‚Äôs the second most used word in the Jane Austen books! Take that Jane Austen!\nActually, it‚Äôs also the first word for Bront√´ and Horwell, isn‚Äôt that interesting?\nWant to do this yourself? The core of it is using unnest_tokens() to create one word per row and count() to tally them up."
  },
  {
    "objectID": "blog/notetaking-part2.html#chapter-3-what-is-the-sentiment-of-my-writing",
    "href": "blog/notetaking-part2.html#chapter-3-what-is-the-sentiment-of-my-writing",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "Knowing what I write about is interesting, but the next logical step is to figure out how I felt when I was writing it. This is where sentiment analysis comes in. There are several different methods, or ‚Äúlexicons,‚Äù available in R that assign positive or negative scores to words. I tried many using my dataset, but for simplicity, here I will use the one called ‚ÄúAFINN‚Äù.\nThe AFINN lexicon is essentially a word list where each word is rated on a scale from -5 (very negative) to +5 (very positive). The process is quite simple: the code goes through each words in my journal entries, finds all the words that exist in the AFINN list, and then adds up their scores. An entry with words like ‚Äúamazing‚Äù and ‚Äúhappy‚Äù would get a high positive score, while an entry with ‚Äúterrible‚Äù and ‚Äústress‚Äù would get a negative one.\nThe get_sentiment() function from the syuzhet package does all this work for you, creating a single sentiment score for each day‚Äôs entry.\nThe first thing I did was plot the sentiment score of my entries over time. This is where the story took another sharp turn.\n\nlibrary(syuzhet)\n\nsentiment &lt;- entries %&gt;%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Plot sentiment over time\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n\n\nIs it my impression or is there a bit of a gap? How come?\nThen I looked at the dates and realised: that‚Äôs when I was writing my dissertation! I remember now, the tag I was using during that time was #Dissertation Journal, to distinguish it from other entries. I should add those entries too. Well, easy enough, I could re-run my loop, adding an additional tag!\n\nextract_entries &lt;- function(file_path) {\n  lines &lt;- readLines(file_path, warn = FALSE)\n  file_date &lt;- str_extract(basename(file_path), \"\\\\d{4}_\\\\d{2}_\\\\d{2}\") %&gt;% ymd()\n  \n  result &lt;- list()\n  i &lt;- 1\n  \n  while (i &lt;= length(lines)) {\n    line &lt;- lines[i]\n    if (str_detect(line, \"moodlog|journaling|Dissertation Journal\")) { #Add the additional tag!\n      \n      matched_tag &lt;- str_extract(line, \"moodlog|journaling|Dissertation Journal\") #Here too!\n      entry_lines &lt;- c(line)\n      i &lt;- i + 1\n      \n      while (\n        i &lt;= length(lines) &&\n        (str_detect(lines[i], \"^\\\\s\") && !str_detect(lines[i], \"^\\\\s*-\\\\s?#moodlog|#journaling\"))\n      ) {\n        entry_lines &lt;- c(entry_lines, lines[i])\n        i &lt;- i + 1\n      }\n      \n      # Save entry\n      result &lt;- append(result, list(tibble(\n        date = file_date,\n        file = basename(file_path),\n        tag = matched_tag,\n        entry_text = paste(entry_lines, collapse = \"\\n\")\n      )))\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  if (length(result) &gt; 0) {\n    bind_rows(result)\n  } else {\n    NULL\n  }\n}\n\nentries &lt;- map_df(md_files, extract_entries)\n\nentries &lt;- entries %&gt;%\n  mutate(\n    entry_text = entry_text %&gt;%\n      str_remove(\"journaling|moodlog|Dissertation Journal\") %&gt;%\n      str_remove_all(\"\\\\n|\\\\t\") %&gt;%\n      str_remove_all(\"-|#|\\\\|\") %&gt;%\n      str_remove_all(\"\\\\[\\\\[.*?\\\\]\\\\]\") %&gt;%\n      \n      str_remove_all(\":LOGBOOK:.*?:END:\") %&gt;%\n      str_remove_all(\"logseq.orderlisttype::\") %&gt;% \n      \n      str_remove_all(\"collapsed:: true\") %&gt;%\n      str_remove_all(fixed(\"{{renderer :wordcount_}}\")) %&gt;% \n      \n      str_squish()\n  )\n\nsentiment &lt;- entries %&gt;%\n  mutate(\n    # Using 'afinn'\n    sentiment_score = get_sentiment(entry_text, method = \"afinn\")\n  )\n\n# Re-plot\nggplot(sentiment, aes(x = date, y = sentiment_score)) +\n  geom_line(color = \"purple\", alpha = 0.5) +\n  geom_point(aes(colour = tag)) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Date\", y = \"Sentiment Score\")\n\n\nThis seems about right! I am happy I left the tags in the dataset now, I could see the evolution of my journaling, from moodlog to journaling, with a big sidetrack project in the middle!\nThe tag column could also have a new meaning. I could now FOR EXAMPLE visualise if I was more prone to use negative words while writing my dissertation (no, we are not going there. It was just a thought).\nBut with these new entries, what about my most used words now?\n\n# Re-run word frequency with the new tag\n\nwords_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  anti_join(stop_words) %&gt;% \n  count(word, sort = TRUE)\n\nwords_counted  %&gt;% \n  filter(n &gt; 20) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nAfter including my dissertation entries, the word frequency analysis had completely changed. My most used word was not time anymore, but paper.\nLOL. Yes, I literally laughed out loud on this one.\n\n\nTo understand what was driving these scores, I wanted to see the most common positive and negative words. The tidytext book has a fantastic visualization for this: a comparison word cloud.\n\npositive_negative_words &lt;- entries %&gt;%\n  # i need to tokenize into single words\n  unnest_tokens(word, entry_text) %&gt;%\n  inner_join(get_sentiments(\"bing\")) %&gt;%\n  count(word, sentiment, sort = TRUE)\n\n# We need to pivot the data into a matrix format:\n# one row per word, one column for \"positive\", one for \"negative\".\n\npositive_negative_words %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  comparison.cloud(\n    colors = c(\"firebrick\", \"steelblue\"),\n    max.words = 100\n  )\n\n#LOL shrew is a NEGATIVE word XD \n\n\nThis is where things get both interesting and messy.\nApparently ‚Äúshrew‚Äù is a negative word! I might need to thank Shakespeare for that. But when the shrew is your actual study species, the word doesn‚Äôt quite have the same meaning as in The Taming of the Shrew, right?\nBut I also see other problems. For example, ‚Äúexcel‚Äù could mean the verb, or it could mean I was frustrated with Microsoft Excel. ‚ÄúChronic‚Äù and ‚Äústress‚Äù likely refer to a study I was working on about chronic stress, not my chronic stress.\nAnd what about ‚Äúregression‚Äù? I am sure I never meant it negatively, I was probably talking about statistics (wait, that does not exclude the negative connotation tho!)\nIt‚Äôs fascinating. Have you ever wondered how many scientific terms have a negative meaning? ‚ÄúIssue‚Äù, ‚Äúregression‚Äù, ‚Äúplot‚Äù‚Ä¶ it‚Äôs a long list!\nI‚Äôm also amused by how often the word ‚Äúlike‚Äù appears on the positive side. Am I using it too much in my writing?"
  },
  {
    "objectID": "blog/notetaking-part2.html#chapter-4-the-emotional-rhythm-of-my-week",
    "href": "blog/notetaking-part2.html#chapter-4-the-emotional-rhythm-of-my-week",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "After wrestling with the beautiful mess of the word cloud, I could finally stop looking at individual words and start searching for broader patterns in my own emotional week.\nWas there a rhythm to my moods? A secret logic to my good and bad days?\nThis is where Logseq‚Äôs structure really shines. Since every journal entry is automatically tied to a daily note, getting the day of the week is incredibly simple with the wday() function from the lubridate package. I decided to look at the average sentiment for each day of the week, and that‚Äôs when I found it. A clear, undeniable pattern.\nApparently, my week has a villain, and its name is Friday.\n\nlibrary(lubridate) # For wday()\n\n# It should have columns for 'date' and 'sentiment_score'\n\nworkweek_sentiment &lt;- sentiment %&gt;%\n  mutate(day_of_week = wday(date, label = TRUE, week_start = 1)) %&gt;%\n  group_by(day_of_week) %&gt;%\n  filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %&gt;% #let's remove saturday and sunday!\n  summarise(\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(workweek_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n\nweek_sentiment &lt;- sentiment %&gt;%\n  mutate(\n    # Get the day of the week from the date\n    day_of_week = wday(date, label = TRUE, week_start = 1)\n  ) %&gt;%\n  group_by(day_of_week) %&gt;%\n  #filter(!(day_of_week %in% c(\"Sa\", \"So\"))) %&gt;% #let's remove saturday and sunday!\n  summarise(\n    # Calculate the average sentiment for each day\n    avg_sentiment = mean(sentiment_score, na.rm = TRUE),\n    entry_count = n()\n  )\n\nggplot(week_sentiment, aes(x = day_of_week, y = avg_sentiment, fill = day_of_week)) +\n  geom_col(show.legend = FALSE) +\n  #geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Day of the Week\",\n    y = \"Average Sentiment Score\"\n  )\n\n\n\nSide note: If you are wondering about the weeknames, just know they are in German!\nMy mood consistently takes a nosedive at the end of the work week, hitting a low on Friday. But the story gets more interesting when you look at the weekend. The sentiment shoots up dramatically on Sundays. The contrast was so stark that I ended up making two plots: one showing just the weekdays, and another including the weekend for comparison.\nIt was one of those moments where data confirms a vague feeling you‚Äôve had for years.\nWho knew? Well, a part of me did, I guess. But seeing it as a data point makes it real. It‚Äôs no longer just ‚Äúa bad day‚Äù; it‚Äôs a predictable rhythm. And once you see the pattern, you can start to do something about it. This is a discovery I can now be mindful of: I could, for example, schedule lighter tasks for Fridays or just be a little kinder to myself when the end-of-week slump hits.\n\n\nAfter looking at my main themes, I got curious. What about the stop_words? Could they tell a story too?\nI started thinking about it when I saw the word ‚Äúlike‚Äù so often repeated. I am not sure I use it in my writing as a verb, but more like‚Ä¶ well, exactly like that. I would find it interesting to investigate which stop words I use more often, and if I should rethink the way I write.\n\n# Find and count only the stop words\nstop_words_counted &lt;- entries %&gt;%\n  unnest_tokens(word, entry_text) %&gt;%\n  # Keep only the words that are in the stop_words list\n  inner_join(stop_words, by = c(\"word\" = \"word\")) %&gt;%\n  count(word, sort = TRUE)\n\nstop_words_counted %&gt;% \n  filter(n &gt; 50) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\n\nOk, using the whole stop-words dataset might not have much information. The fact that ‚ÄúI‚Äù and ‚Äúthe‚Äù are the most used ones doesn‚Äôt tell me much on its own.\nBut I could filter out of this dataset. For example, I started with modal verbs. I wanted to see if I was a person of ‚Äòshoulds,‚Äô ‚Äòcoulds,‚Äô or ‚Äòwants.‚Äô Then I went over the pronouns, then conjunctions and time transition words.\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"should\", \"could\", \"would\", \"will\", \"want\", \"shall\", \"can\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"i\", \"me\", \"my\", \"myself\", \"you\", \"your\", \"he\", \"she\", \"they\", \"we\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"but\", \"however\", \"although\", \"because\", \"so\", \"therefore\", \"then\")) %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()\n\nstop_words_counted %&gt;% \n  filter(word %in% c(\"before\", \"after\", \"during\", \"since\", \"until\", \"ago\", \"within\"))  %&gt;% \n  mutate(word = reorder(word, n)) %&gt;% \n  ggplot(aes(n,word)) +\n  geom_col()"
  },
  {
    "objectID": "blog/notetaking-part2.html#chapter-5-the-road-not-taken-aka-other-methods",
    "href": "blog/notetaking-part2.html#chapter-5-the-road-not-taken-aka-other-methods",
    "title": "Is Note-Taking as powerful as I think? (2/2)",
    "section": "",
    "text": "The ‚ÄúText Mining with R‚Äù book is full of other amazing techniques, and I tried a couple more.\nI first tried word correlation to see which words I tend to associate with each other. The results were interesting but didn‚Äôt tell me anything I didn‚Äôt already expect (yes, dissertation is correlated with paper or writing).\nNext, I tried topic modeling to see if an algorithm could automatically find the hidden themes in my journal. This didn‚Äôt work as well as I‚Äôd hoped, likely because my journal entries are often short and context-heavy.\nChecking numbers with bigram was another interesting topic: I had a lot of numbers in my entries (often coupled with ‚Äúminutes‚Äù or ‚Äúweeks‚Äù, or maybe plain schedules) and I tried to couple numbers with their words, but that didn‚Äôt work: I somehow filtered some information from my dataset (the shrews ID from my dissertation have are unique number combinations) so that was a bit of a bummer.\nWhile these didn‚Äôt produce mind-blowing results for my specific dataset, they are incredibly powerful techniques. If you want to learn more, I highly recommend checking out the chapters on word correlation and topic modeling in the ‚ÄúTidy Text Mining with R‚Äù book!\n\n\nI approached this project as a beginner, wanting to see if I could find a story in the massive amount of text in my Logseq vault. The mission was biased, of course; I wanted to learn something about myself.\nAnd I did. I learned that my focus shifted dramatically during my dissertation, that my mood has a weekly rhythm, and that the tools to find these insights are accessible to anyone willing to learn.\nWe all have a massive, unstructured dataset of our own lives in our notes, emails, or calendars. It might look like junk, but it‚Äôs also a goldmine. The tools to explore it are more accessible than ever, and the only prerequisite is a little bit of curiosity.\nSo, is note-taking that powerful? It‚Äôs powerful enough to turn forgotten thoughts into a map of who you are. And for me, that‚Äôs powerful enough."
  },
  {
    "objectID": "blog/quartose.html",
    "href": "blog/quartose.html",
    "title": "TidyTuesday and Quartose",
    "section": "",
    "text": "Full disclosure first: this wasn‚Äôt the blogpost I planned. I had something else in mind for this week (I guess that‚Äôs a future Ceci problem).\nOn Tuesday, I saw a LinkedIn post about a new R package called quartose, created by Danielle Navarro. Like everything with the word Quarto in it, I was hooked, and after reading her blogpost I thought: this looks like fun.\nDid I already say it was Tuesday? wink wink, #TidyTuesday.\nSo I took the opportunity to explore the dataset from this week‚Äôs challenge using quartose. I didn‚Äôt start with a specific analysis in mind, I just wanted to see how quartose felt in a real document."
  },
  {
    "objectID": "blog/quartose.html#why-i-tried-quartose",
    "href": "blog/quartose.html#why-i-tried-quartose",
    "title": "TidyTuesday and Quartose",
    "section": "",
    "text": "Full disclosure first: this wasn‚Äôt the blogpost I planned. I had something else in mind for this week (I guess that‚Äôs a future Ceci problem).\nOn Tuesday, I saw a LinkedIn post about a new R package called quartose, created by Danielle Navarro. Like everything with the word Quarto in it, I was hooked, and after reading her blogpost I thought: this looks like fun.\nDid I already say it was Tuesday? wink wink, #TidyTuesday.\nSo I took the opportunity to explore the dataset from this week‚Äôs challenge using quartose. I didn‚Äôt start with a specific analysis in mind, I just wanted to see how quartose felt in a real document."
  },
  {
    "objectID": "blog/quartose.html#exploring-the-colour-naming-dataset-with-quartose",
    "href": "blog/quartose.html#exploring-the-colour-naming-dataset-with-quartose",
    "title": "TidyTuesday and Quartose",
    "section": "Exploring the colour naming dataset with quartose",
    "text": "Exploring the colour naming dataset with quartose\nThis week‚Äôs TidyTuesday features millions of responses from an online color naming game slash survey. It includes a lot of responses (kudos to the curator of this dataset, Nicola Rennie), along with metadata about users (e.g.¬†self-reported Y chromosome presence, colorblindness etc.).\nRather than focusing on the full dataset, I used a subset to play with structuring and presenting small summaries. The visualizations are extremely simple, but enough to test how different layout elements from quartose could help me guide the narrative.\nYou‚Äôll see a few of these elements below:\n\nSummary tables with side content\nTabs that organize plots by grouping\nNotes placed outside the main column\n\nThe dataset is available in the color project repo, and the TidyTuesday project is a great excuse to learn something new each week (well, that‚Äôs my excuse!)."
  },
  {
    "objectID": "blog/quartose.html#welcome-to-the-color-chaos",
    "href": "blog/quartose.html#welcome-to-the-color-chaos",
    "title": "TidyTuesday and Quartose",
    "section": "Welcome to the color chaos",
    "text": "Welcome to the color chaos\nThousands of people participated in a color-naming survey online. This document explores the funny and, sometimes, messy data behind the responses.\n\nWho responded?\nLet‚Äôs begin with the basics: who were the users behind these color names?\n\n\n\nUser summary\n\n\nTotal Users\nMonitor Types\nUsers with Y\nUsers without Y\nColorblind Users\nAvg. Spam Probability\n\n\n\n\n152401\n3\n103430\n41464\n5588\n0.222\n\n\n\n\n\nOver 150,000 individuals took part in the experiment, each using one of a few different types of monitors. Of them, more than 40,000 were not equipped with a Y chromosome. Over 5,000 individuals indicated they were colorblind. One interesting variable in this dataset is a spam probability score, meaning (I guess) a numeric value that estimates how likely a given user was submitting low-quality answers.\nquarto_div(content = c(paste0(\"Colorblind participants: \", users_summary$n_cb), \"&lt;br&gt;\",\n                       paste0(\"Average spam probability: \", round(users_summary$avg_spam_prob, 3)), \"&lt;br&gt;\",\n                       paste0(\"Monitor types: \", users_summary$n_monitors)),\n  class = \"column-margin\")\n\n\nColorblind participants: 5588  Average spam probability: 0.222 Monitor types: 3\n\n\nHow much data?\n\n\n# A tibble: 1 √ó 3\n  answers named_colors hex_codes\n    &lt;int&gt;        &lt;int&gt;     &lt;int&gt;\n1 1058211          949       949\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you can see, this is a large dataset. People typed in thousands of color names for hundreds of distinct hex codes. That‚Äôs a lot of ways to say ‚Äúblue.‚Äù\n\n\n\nAnswers per user\n\nHistogramAverage by Y chromosomeDensity\n\n\n\n\n\n\n\n\n\n\n\n\nWhat you‚Äôre seeing:\n- The histogram shows that most users gave fewer than 10 answers, with a long tail of power users.  - On average, answer counts per user are fairly similar across Y chromosome groups.  - The density plot shows the shape of participation, all groups are heavily skewed.\n\n\nColorblindness\nColor perception is often assumed to be universal, but of course, it isn‚Äôt.\nLet‚Äôs take a quick look at how many participants reported being colorblind:\n\nusers |&gt; \n  mutate(cb_status = factor(case_when(colorblind == 1 ~ \"Colorblind\",\n                                      colorblind == 0 ~ \"Not colorblind\",\n                                      TRUE ~ \"Unknown\"), levels = c(\"Not colorblind\", \"Colorblind\", \"Unknown\"))) |&gt; \n  count(cb_status) |&gt; \n  ggplot(aes(x = cb_status, y = n, fill = cb_status)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Colorblindness Status\", y = \"Number of users\")\n\n\n\n\nColorblindness status\n\n\n\n\n\n\nSpam Probability\nWhat kind of variation is there in users‚Äô spam scores?\nThis variable is a bit opaque‚Ä¶ I don‚Äôt know exactly how it‚Äôs computed. But it‚Äôs cool to see how it distributes across groups.\n\n\n\n\n\nDistribution of spam scores\n\n\n\n\n\n\n\nDistribution of spam scores\n\n\n\n\n\n\nIs monitor type related?\nCould the type of monitor affect color perception or how people respond?\n\nusers |&gt; \n  mutate(monitor = fct_lump(monitor, n = 3)) |&gt;\n  count(monitor) |&gt; \n  ggplot(aes(x = monitor, y = n, fill = monitor)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Monitor Type\", y = \"Number of users\")\n\n\n\n\nMonitor type vs number of users\n\n\n\n\nI admit, I had to google this. Most users had modern displays (LCD or Liquid Crystal Display), but there were still a few on older CRTs (Cathode Ray Tube, that really sounds like something from Back to the Future) or other types, which might influence how colors looked during the experiment.\n\n\nConclusion\nThere‚Äôs a lot more to explore. But I really liked the unexpected combinations and perspectives people bring when interpreting color.\nquarto_div(content = \"Want to try your own analysis? Check out the full dataset at the TidyTuesday GitHub.\",\n           class = c(\"callout-tip\"))\n\n\n\n\n\n\nTip\n\n\n\nWant to try your own analysis? Check out the full dataset at the TidyTuesday GitHub."
  },
  {
    "objectID": "blog/quartose.html#tldr",
    "href": "blog/quartose.html#tldr",
    "title": "TidyTuesday and Quartose",
    "section": "TL;DR",
    "text": "TL;DR\nTo me, quartose feels a bit like a lightweight take on scrollytelling. Not quite like closeread, which I may have already talked about profusely, but still an interesting way to shape an exploratory narrative.\nDoes it save me time compared to writing plain Quarto syntax? Not really.\nDo I want to try it again? Oh yes.\nI imagine it could be useful for workshop materials or blog tutorials, though I haven‚Äôt settled on how I might use it yet. As Danielle wrote:\n\n‚ÄúAll that quarto_div() actually does is construct syntax for the Quarto parser to capture and render.‚Äù\n\nThat‚Äôs exactly what it does. And for someone already used to Quarto‚Äôs logic, it feels like a natural extension."
  },
  {
    "objectID": "blog/sketchnoting-beginners.html",
    "href": "blog/sketchnoting-beginners.html",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "",
    "text": "At some point between 5 and 7 years old, you learned how to write. I‚Äôm not talking about penmanship (that came later) or writing a novel (that probably didn‚Äôt come yet), but about the simple act of decoding symbols and putting them together to create sounds.\nYou might not remember the process now, but I assure you: you sucked at it. You had to practice for months, years, to get those symbols right. You probably learned cursive too, and that was a whole other level of hell.\nBut you did it. Because you had to, there was no other choice.\nEventually, you learned how to write, how to decode those symbols into sounds, put them together into ideas, and now it‚Äôs just‚Ä¶ happening. You give it no more thought than you would give which keys to hit on the keyboard to spell a word you‚Äôve known for years (what about typos? Typos are typing mistakes, not writing mistakes!)\nWhy am I starting with this topic, when I should be writing about sketchnoting? Because it‚Äôs exactly the same thing.\nSketchnoting is about drawing as much as writing is about creating a literature masterwork. You don‚Äôt need to know how to write a bestseller to write in your daily journal, just as you don‚Äôt need to know how to draw to sketchnote.\nSo, what do you actually need? Practice.\nExactly like you learned these weird symbols that make up our words, and had to practice to make them look readable (penmanship, here it comes), the same thing applies to sketchnoting. The more you practice, the easier it gets.\n(By the way, it‚Äôs the same advice for painting, drawing, knitting, crocheting, programming, learning a new language, taking care of a dog, driving, changing a diaper, or anything else you can think of, but for most of these examples, people still reply, ‚ÄúI am not gifted‚Äù or ‚ÄúI could never do it like you do it.‚Äù. Sounds familiar?)\nThen, I hope this blog post hits closer to the soul of whomever needed to know this. After reading this, ‚ÄúI can‚Äôt draw, so I can‚Äôt sketchnote‚Äù won‚Äôt work as an excuse anymore.\nReady?"
  },
  {
    "objectID": "blog/sketchnoting-beginners.html#the-science-behind-why-it-works",
    "href": "blog/sketchnoting-beginners.html#the-science-behind-why-it-works",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "The Science behind why it works",
    "text": "The Science behind why it works\nConsider a talk or a presentation. The speaker is talking quickly, and there‚Äôs no way you can write down everything they say word for word. With traditional notes, you‚Äôd try and fail, only capturing fragments. When you review your notes, you miss the connection between these fragments, or you might fail to remember what was the endpoint of a very convoluted story. With sketchnoting, you‚Äôre immediately prioritizing and simplifying as you listen. This act of active processing the speaker‚Äôs complex ideas into your own personal language of keywords and symbols is helping you learn while you listen, instead of passively writing words down.\nThis concept is called Dual-Coding Theory. Our brains process information through two distinct channels: a verbal channel for words and a visual channel for images. When you take traditional notes, you‚Äôre primarily using only the verbal channel. But when you sketchnote, you‚Äôre intentionally using both at the same time. This creates two ‚Äúmental traces‚Äù for the same piece of information, one verbal and one visual. Because the memory is stored in two places, it‚Äôs much stronger and easier to retrieve later on.\nAt the same time, we all know that a picture is worth a thousand words. This is called the Picture Superiority Effect, and it‚Äôs a well-established cognitive principle that shows we remember concepts much better when they are presented as images rather than words.\nTo sum up, by using both words and pictures, you‚Äôre giving your brain two different ways to remember the same information! &lt;br. So, how do you get started?"
  },
  {
    "objectID": "blog/sketchnoting-beginners.html#the-tools-you-already-have-and-the-skills-you-will-realize-you-have",
    "href": "blog/sketchnoting-beginners.html#the-tools-you-already-have-and-the-skills-you-will-realize-you-have",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "The tools you already have (and the skills you will realize you have)",
    "text": "The tools you already have (and the skills you will realize you have)\nThe only tool you need in this adventure is something you have with you in every situation: your brain.\nWhile I keep repeating that you don‚Äôt need to be an artist (and I believe it), you do need to flex three key skills that have nothing to do with drawing:\n\nActive Listening: A speaker talks at about 150 words per minute. That‚Äôs way too fast to write everything down, which is a good thing! Sketchnoting forces you to let go of the idea of capturing every word and, instead, to truly listen for the core concepts.\nPrioritizing: As you listen, your brain will naturally start to filter. What‚Äôs the main point? What‚Äôs a supporting example? What‚Äôs just an interesting anecdote? This skill of prioritizing is crucial for keeping your sketchnote clear and focused.\nSynthesizing: This is the most important skill. It‚Äôs the ability to take a complex idea and distill it down to its essence. When you sketchnote, you are constantly asking yourself, ‚ÄúHow can I represent this idea with a single word and a simple picture?‚Äù\n\nThese are the real muscles you‚Äôll be building. A fun side note? They will make you a better learner and problem-solver."
  },
  {
    "objectID": "blog/sketchnoting-beginners.html#how-do-i-do-it-a-sketchnoting-case-study",
    "href": "blog/sketchnoting-beginners.html#how-do-i-do-it-a-sketchnoting-case-study",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "How do I do it: a Sketchnoting Case Study",
    "text": "How do I do it: a Sketchnoting Case Study\nMy biggest roadblock when I started was ‚Äúwhat do I even draw?‚Äù For me, the answer came from preparing for a conference. I knew I couldn‚Äôt capture every detail from every talk, so I needed a system. I skimmed the talk submissions, looked at the titles and abstracts, and made a list of the key concepts and model species that seemed most important. This manual process helped me build a targeted visual vocabulary before I even stepped into the room. \n\n\n\n\n\n\nMy notebook showing the practice sketches for the EMBO Conference. And a cappuccino.\n\n\n\n\n\n\n\nClose-up of my notebook, on the morning of the first day of the EMBO Conference. The page shows repeated icons of main concepts of the conference.\n\n\n\n\n\n As a case study for this blog post, I wanted to show you how to do it yourself with that same approach. For this occasion, I chose a topic I have no real knowledge, but always wanted to know more about: Urban Landscaping.\n\nStep 1: Scan for keywords\nFind an article or a short video on your chosen topic. For a conference, start from the program; for a series of lectures, you might have some PDF or documents beforehand. If you have nothing, a google search or the wikipedia page will do just fine.\nRead through it once, not to understand every detail, but to identify the most important, specific terms. Look for words that are:\n\nIn the title or headings.\nBolded or italicized.\nRepeated often.\nNouns that seem central to the concept.\n\nIf you are following along, here are the resources I used: How Urban Landscapes are saving you and the planet: 5 Benefits of Urban Landscape Architecture, and Urban Landscaping: Transforming City Spaces in 2024 Here are the concepts I wrote down:\n\nThis instantly gave me a list of the specific concepts I needed to focus on to start building my visual vocabulary.\n\n\nStep 2: Develop the icons\nOnce you have your list, it‚Äôs time to make your visual language. Remember, these are for you, so they don‚Äôt have to be perfect. The goal is simple, quick, and understandable.  Here‚Äôs what I came up with:\n\nAs you can see, I made some icons multiple times. Maybe I started with an idea, then I tried to draw it once or twice, and I realised if it was working or not, or if it was wasy to make, or if it was fast enough.  If a concept does not mean anything to you, or it does not spark any icon in your mind, feel free to google it!  For example, ‚Äúbiophilic design‚Äù was a completely new concept for me. I googled ‚Äúbiophilic design icon‚Äù, and I found some icons with a building and a leaf next to it, to symbolize the integration of botanical elements and urban items. If you check the lower part of the right page of my notebook, you can see that I started from a skyscraper with plants on the roof (too complex), then I tried a window (didn‚Äôt get the message through), then a house with a leaf (fast, simple).  Is it the best option? Probably not. But if I look at it, do I remember ‚Äúbiophilic design‚Äù? Yes!  That‚Äôs exactly the point of it! Maybe you might have a different idea, or you are faster at making a skyscraper. The key was to make each sketch meaningful to you, not to anyone else. This is where your brain takes over and the system comes to life.\n\n\nStep 3: Basic sketchnoting items\nBeyond content-specific icons, there is a universal visual language that goes beyond the topic being discussed. These can be, for example, arrows, basic shapes (circles, rectangles, cloud), and your own handwriting! I tried to summarise them like this:\n\nContainers: Use rectangles, circles, or clouds to group related ideas. For titles, use a banner or a larger, bolder font.\nConnectors: Lines, arrows, dots, dotte lines‚Ä¶ all of those can show relationships, cause-and-effect, a sequence of steps, or maybe something yet to explore.\nTypography: Even tho I think you should not worry about perfect handwriting, you can play different letter sizes and styles to create a visual hierarchy. You can start small, for example with bold or all caps titles, and smaller body text.\n\n\n\nPutting it all together: your first sketchnote\nYou have your tools, your icons, and your grammar. Now what? You have to jump in and do it.\n\nChoose a short, simple piece of content. A 5-10 minute TED Talk, a short article, or a chapter of a book is perfect.\nListen or read for the main idea. Don‚Äôt try to capture every single word. Focus on key concepts and keywords.\nTranslate those keywords into your icons and add them to your page. Use your containers and connectors to structure the information as you go.\nDon‚Äôt worry about making it perfect! Your goal is to process the information, not to win an art contest. It will be messy. It will be imperfect. And that is exactly what makes it a useful learning tool. \n\nFor the purpose of this blogpost, I looked on youtube a very short talk about *Urban Landscaping, and I found this one. I made a fast digital sketchnote of what I heard, I discarded some information and I focused on what I thought important.\nHere‚Äôs the final piece!"
  },
  {
    "objectID": "blog/sketchnoting-beginners.html#from-practice-to-performing",
    "href": "blog/sketchnoting-beginners.html#from-practice-to-performing",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "From practice to performing",
    "text": "From practice to performing\n‚ÄúWait a minute, sketchnoting is supposed to happen live, but you just told me to practice ahead of time.‚Äù, if you‚Äôre thinking this, you are right to be confused. So which one is it? For me, it‚Äôs a bit of both.  The reality is that I, personally, do not use sketchnoting often. I do when I know I‚Äôm gonna be needing it. For the rest of the time (e.g.¬†lab meetings, steering committee meetings, meetings with students) I am happy to take digital, or handwritten, notes. Since I do not practice sketchnoting often, I know that my sketchnoting muscle are not always trained. That‚Äôs why I practice this 3-steps preparation before the live event. By doing this, I know that the right icons will come to mind when I need it, because I practiced them. \nLet‚Äôs take back the ‚Äúlearning to write‚Äù example. You didn‚Äôt start by writing a story, but by practicing each letter of the alphabet until those symbols became second nature. You built a vocabulary of letters so that when it was time to write, you could focus on the ideas, not the individual shapes. \nBy doing the foundational work now, you are building a library of icons in your brain. This means that when a speaker at a conference says a key term, your hand will knows what to do. By doing it with topics you are familiar with, and often, and with enough practice, you‚Äôll reach a point where you can do it live, and effortlessly, which frees you up to actively listen to the next idea."
  },
  {
    "objectID": "blog/sketchnoting-beginners.html#recommended-resources-to-learn-more",
    "href": "blog/sketchnoting-beginners.html#recommended-resources-to-learn-more",
    "title": "So, you want to start Sketchnoting (but don‚Äôt know how)",
    "section": "Recommended resources to learn more",
    "text": "Recommended resources to learn more\nIf you want to dive deeper, here are some fantastic resources that have helped me get started.\n\nSketchnotes 101: A full beginner-friendly tutorial that does a better job than mine at showing what sketchnoting is and can be\nThe Sketchnote Handbook by Mike Rohde: Widely considered the bible of modern sketchnoting. This book breaks down the process with tons of examples.\nBlah Blah Blah: What To Do When Words Don‚Äôt Work by Dan Roam: While not strictly about sketchnoting, this book will help you think about visual communication and why it‚Äôs so powerful.\nVerbal to Visual: A great resource with articles and videos by Doug Neill about the art and science of visual note-taking.\nEva-Lotta Lamm Blog: One of my favorite sketchnoter, she also have a cool Domestika course for beginners!"
  }
]